{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3c78b8-6c84-4785-b91c-e879040c296d",
   "metadata": {},
   "source": [
    "## Bringing your own Pytorch Model to Rasterflow\n",
    "\n",
    "This guide will show you how you can export a Pytorch model, store in s3, and then run that model in rasterflow.\n",
    "\n",
    "We'll cover these steps using minimal dependencies and a simple toy Pytorch model. This will let us focus on the steps for model export, storing models, and referencing models in rasterflow.\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "To see other examples of exported models and their export scripts, check out our [Huggingface page](https://huggingface.co/collections/wherobots/wherobotsai-models).\n",
    "\n",
    "For a deeper dive on Pytorch's PT2 export format and export methods check out the following tutorials.\n",
    "\n",
    "1. [Torch Export Common Challenges and Solutions](https://docs.pytorch.org/tutorials/recipes/torch_export_challenges_solutions.html) (Beginner)\n",
    "2. [Torch Export Tutorial](https://docs.pytorch.org/tutorials/intermediate/torch_export_tutorial.html) (Advanced, if you want to export complex models or optimize models for different accelerators)\n",
    "3. [PT2 Archive Format Walkthrough](https://docs.pytorch.org/docs/stable/export/pt2_archive.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22059309-fee9-43ad-af79-567f88ba332d",
   "metadata": {},
   "source": [
    "We'll first make an example toy model that matches the input and output of the [Meta and WRI Canopy Height Model](https://huggingface.co/wherobots/meta-chm-v1-pt2).\n",
    "\n",
    "For this regression model, the input is a Pytorch Tensor of continuous values (image data) and the output is Pytorch Tensor of continuous values (canopy height in meters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79bdf4-e57c-4afa-8be6-a8e0ff540ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install torch==2.8 torchvision --extra-index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8016b00-8883-4685-b0e7-492006756250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and load checkpoint\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ExampleModel(nn.Module):\n",
    "    def forward(self, x):\n",
    "        predictions = torch.randn(x.shape)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e03ef-e14b-453e-b3d0-b952a4279044",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,3,256,256)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede5cfc-fb8c-4ce8-a15b-035bff5d6958",
   "metadata": {},
   "source": [
    "Now that we have our model defined, we can run it on some test data and then export it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb86078-8883-4fff-afd1-b21f6b3419e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExampleModel()\n",
    "result = model(x)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05996394-1998-4b32-a24a-026f0c059a56",
   "metadata": {},
   "source": [
    "## Pytorch 2's export formats\n",
    "\n",
    "Pytorch has multiple export formats, each suited for different use case, like storing model weights, model training, inference on edge devices, and inference on servers.\n",
    "\n",
    "If you've been using Pytorch for a while, you may be familiar with it's ubiquitous model checkpoint format, often saved with the extension `.pth`.\n",
    "\n",
    "```\n",
    "PATH = \"model.pth\"\n",
    "torch.save(model.state_dict(), PATH)\n",
    "```\n",
    "\n",
    "This format only stores a simple dictionary of a model's weights, not the structure that loads the model weights or any description of how the model is executed. In order to load the model, one needs to have all the dependencies used to expor tthe model, a burdensome requirement.\n",
    "\n",
    "```\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "```\n",
    "\n",
    "What's more is that it is not possibly to optimize models for specific accelerators using the `state_dict` export method. This makes it difficult to deploy models exported using `torch.save`. \n",
    "\n",
    "## .pt2, an export format for all use cases\n",
    "\n",
    "An improved alternative export method is to use `torch.export`. Torch's export module allows you to export a single model artifact that can be used for both training and inference. It has a number of useful features\n",
    "\n",
    "1. Supports exporting the model on CPU, and then at runtime, moving the model's paramters to GPU.\n",
    "2. Exported models can be compiled to run faster and consume less memory on particular acclerators (CUDA, AMD, or Intel GPUs).\n",
    "3. Standardizes how inference and tryining hyperparamters (model metadata) and model input or output transforms are stored alongside the model.\n",
    "\n",
    "We'll now export our `ExampleModel` in PT2 format. We'll also handle the common case of input preprocessing by exporting our input transforms as a torch nn.Module int he same PT2 archive as our model. `torch.nn.Sequential` can be used to chain together multiple transforms. `torch.nn.Sequential` also helps us use `torch.export` since it has a simple `forward` method that takes a single argument called `input`. This let's us define fewer arguments later when we call `torch.export.export`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36dee7-0f24-487d-97ef-d13fb440b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torchvision.transforms.v2 as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e146e-4efc-41d3-89cc-15a2c452b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_band_mean= [.5]*len(x)\n",
    "per_band_std = [.1]*len(x)\n",
    "norm_transform = torch.nn.Sequential(T.Normalize(mean=per_band_mean, std=per_band_std))\n",
    "print(isinstance(norm_transform, nn.Module))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff5ce9-c44f-4167-a41c-656b07871a53",
   "metadata": {},
   "source": [
    "Next, for the PT2 torch.export method, we need to tell torch about the expected shape of our model input. This helps torch export static file that captures the structure of the model, which can be dependent on the input data. `torch.export` introduces a `Dim` object, which we can use to denote if a dimension is meant to by dynamic, meaning it can be any value greater than 1, or static, meaning that the dimension must be a certain size.\n",
    "\n",
    "Frequently asked questions the topic of data dependent export include:\n",
    "1. How do I know what shape my model should take?\n",
    "   * Answer: The input shape to a model can change the runtime performance and accuracy of the model result. It's best to check the recommendations from the model creators.\n",
    "3. Should I always just use dynamic for all dimensions?\n",
    "   * Answer: Usually no. Many models have logic that requires dimensions to be fixed. An exmple of this is some detection models that have lots of control flow that depends on the shape of the input. This control flow can sometimes restrict the model to being exported on the same patch height and width that a model was trained on.\n",
    "\n",
    "A good rule of thumb is that it is usually safe to leave the batch size dimension dynamic, while fixing the channel, height, and width shape dimensions. If you'd like to support the ability to export with dynamic shapes besides the batch dimension, it's quick trial and error to try exporting with these settings.\n",
    "\n",
    "We'll export our toy model with a dynamic batch size, fixed channel dimension of 3, and fixed height and width of 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a83526-f1e5-41db-81f6-3905584da759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.export import Dim\n",
    "input_shape_constraints = [-1, 3, 256, 256]\n",
    "example_input_shape = [2, 3, 256, 256]\n",
    "example_tensor = torch.randn(*example_input_shape, requires_grad=False)\n",
    "dims = tuple(Dim.AUTO if dim == -1 else dim for dim in input_shape_constraints)\n",
    "print(dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed8352-e7c0-42ed-a13c-7a151703aeef",
   "metadata": {},
   "source": [
    "Next we will set our model to \"eval\" mode before we export and set some expected parameters for our model, including the device type it will run on and the expected data type of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee11e4-1ffc-4440-a4f1-2b6b33ed5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "device = torch.device(\"cuda\")\n",
    "dtype = torch.float32\n",
    "model.eval()\n",
    "model = model.to(device).to(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da07463-a44c-4191-a302-d07b9f054a50",
   "metadata": {},
   "source": [
    "torch.export also must know the expected arguments for the forward pass function for our torch model. We can parse this from the nn.Module with python's built-in `inspect` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a9869-7892-4033-b2cf-2961927647e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arg = next(iter(inspect.signature(model.forward).parameters))\n",
    "print(model_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6493a990-19e6-4f7e-9337-27e8d715c2bb",
   "metadata": {},
   "source": [
    "And now we are ready to export our model to an ExportedProgram! An Exported Program is an in-memory model object that can be stored within a `.pt2` archive file on disk.\n",
    "\n",
    "This ExportedProgram stores many things related to our model, including the `state_dict` for containing model weights and `example_inputs` which can be handy for testing a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a1cd4b-286c-465f-98b8-f0e1c8fc5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_program = torch.export.export(mod=model, args=(example_tensor,), dynamic_shapes={model_arg: dims})\n",
    "print(model_program.example_inputs[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d66be2-540a-4d7b-84ae-825f2cd6c8bc",
   "metadata": {},
   "source": [
    "We'll follow the same export steps for our transforms module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57054f33-af49-4e92-adea-34f6e06c1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_transform.eval()\n",
    "norm_transform = norm_transform.to(device).to(dtype)\n",
    "transform_arg = next(iter(inspect.signature(norm_transform.forward).parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887e495-c4fe-4e41-b335-4888956c2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_program = torch.export.export(\n",
    "    mod=norm_transform, args=(example_tensor,), dynamic_shapes={transform_arg: dims}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378be3c7-f471-4541-a94c-a9c6d3e4f6e9",
   "metadata": {},
   "source": [
    "Now that we have ExportedPrograms for our model and normalize transform, we are ready to save the model to a `.pt2` archive.\n",
    "\n",
    "Note: There are a few ways to save `.pt2` model archives with torch. `torch.export.save` can be used to save a single ExportedPgrogram. We will instead use `torch.export.pt2_archive._package.package_pt2` to save the transforms and model ExportedProgram into one `.pt2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d113f364-63d4-4d26-bf51-e02da1b59002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.export.pt2_archive._package import package_pt2\n",
    "exported_programs = {}\n",
    "local_model_path = \"example.pt2\"\n",
    "exported_programs[\"model\"] = model_program\n",
    "exported_programs[\"transforms\"] = transforms_program\n",
    "\n",
    "package_pt2(\n",
    "    f=output_file,\n",
    "    exported_programs=exported_programs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d5a14d-3199-4484-8529-1697d63b3acc",
   "metadata": {},
   "source": [
    "## Running the Custom Model with RasterFlow\n",
    "\n",
    "In addition to using Wheorbots Hosted Models, RasterFlow allows you to run inference using your own custom models like the one we have just exported.\n",
    "\n",
    "To do this, we first need to upload the model (in this case, a `.pt2` file) to a location on S3 that RasterFlow can access. We will use the Wherobots Managed Storage bucket that comes with each account. We will then define an `InferenceConfig` that tells the RasterFlow how to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7cb39-3ab4-4b44-8d5c-3db7b5a379a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(profile=\"default\")\n",
    "\n",
    "# Define the destination path on S3\n",
    "# We use the USER_S3_PATH environment variable to ensure it goes to your personal bucket space\n",
    "s3_model_path = os.getenv(\"USER_S3_PATH\") + local_model_path\n",
    "fs.put(local_model_path, s3_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab871aa-fbba-42b2-9ed1-667ae00b7d1f",
   "metadata": {},
   "source": [
    "## Defining the RasterFlow InferenceConfig\n",
    "\n",
    "Now that the model weights are on S3, we need to define the configuration for the inference job.\n",
    "\n",
    "Note: If you are using a Wherobots Hosted Model, there's no need for this step, as all of these models are available as preconfigured ModelRecipes.\n",
    "\n",
    "See the [InferenceConfig](https://docs.wherobots.com/reference/rasterflow/data-models#inferenceconfig) documentation for details on these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def373a2-42b9-4c50-a57b-7838632bf52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterflow_remote.data_models import InferenceConfig, InferenceActorEnum, MergeModeEnum, ResamplingMethod\n",
    "\n",
    "custom_inference_config = InferenceConfig(\n",
    "    model_path = s3_model_path,\n",
    "    actor = InferenceActorEnum.REGRESSION_PYTORCH,\n",
    "    patch_size = 224,\n",
    "    clip_size = 28,\n",
    "    device = \"cuda\",\n",
    "    features = [\"red\", \"green\", \"blue\"],\n",
    "    labels = [\"canopy_height\"],\n",
    "    max_batch_size=64,\n",
    "    merge_mode = MergeModeEnum.WEIGHTED_AVERAGE\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86e159-7b45-47fc-b287-d2daea6a4c5a",
   "metadata": {},
   "source": [
    "We can build the input for our model like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b96a00-32eb-4840-b63d-7a125990e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wkls\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "# Generate a geometry for Nashua, NH using WKLS (https://github.com/wherobots/wkls)\n",
    "gdf = gpd.read_file(wkls.us.nh.nashua.geojson())\n",
    "\n",
    "# Save the geometry to a parquet file in the user's S3 path\n",
    "aoi_path = os.getenv(\"USER_S3_PATH\") + \"nashua.parquet\"\n",
    "gdf.to_parquet(aoi_path)\n",
    "\n",
    "from rasterflow_remote import RasterflowClient\n",
    "from rasterflow_remote.data_models import InferenceConfig, InferenceActorEnum, MergeModeEnum, ResamplingMethod\n",
    "client = RasterflowClient(mosaics_version= \"v0.18.0\", rasterflow_version=\"v1.40.2\")\n",
    "\n",
    "\n",
    "client.build_gti_mosaic(\n",
    "        gti = \"s3://wherobots-examples/rasterflow/indexes/naip_index.parquet\",\n",
    "        aoi = aoi_path,\n",
    "        bands = [\"red\", \"green\", \"blue\", \"nir\"],\n",
    "        location_field = \"url\",\n",
    "        crs_epsg = 3857,\n",
    "        xy_chunksize = 1024,\n",
    "        query = \"res == .6\",\n",
    "        requester_pays = True,\n",
    "        sort_field = \"time\",\n",
    "        resampling = ResamplingMethod.NEAREST,\n",
    "        nodata= 0.0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
