{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d13cee5-6bbe-44fa-b209-d27d287a86ac",
   "metadata": {},
   "source": [
    "![](https://wherobots.com/wp-content/uploads/2023/12/Inline-Blue_Black_onWhite@3x.png)\n",
    "\n",
    "# WherobotsAI Raster Inference - Bring Your Own Model\n",
    "\n",
    "WherobotsAI Raster Inference supports running your own machine learning models on raster images in order to gather insights using the [Machine Learning Model Extension Specification](https://github.com/stac-extensions/mlm) (MLM). MLM is the standard for discovering, sharing, and running machine learning models for geospatial data.\n",
    "\n",
    "Generally, bringing your own model involves the following steps:\n",
    "\n",
    "* Saving your model checkpoint using Torchscript (through either scripting or tracing).\n",
    "* Choosing an S3 bucket to store your model.\n",
    "* Uploading your Torchscript model to your S3 bucket.\n",
    "* Filling out two [MLM Specification](https://github.com/stac-extensions/mlm) forms (the Asset form and the MLM form) for your model.\n",
    "* Uploading the MLM JSON file to your S3 bucket.\n",
    "* Executing raster inference.\n",
    "* Analyzing your model inference results.\n",
    "\n",
    "## Capabilities\n",
    "\n",
    "WherobotsAI Raster Inference currently supports:\n",
    "\n",
    "* The following computer vision tasks:\n",
    "    * Single-label scene classification\n",
    "    * Object detection\n",
    "    * Semantic segmentation\n",
    "    * Segment Anything 2 (text prompt to polygons)\n",
    "* Workloads with single input tensor and single output tensor\n",
    "* NVIDIA GPU acceleration\n",
    "* Pytorch export formats: Torchscript models, ExportedPrograms, and AOTInductor models\n",
    "\n",
    "### Job Runs\n",
    "\n",
    "You can complete raster inference with WherobotsAI within a Job Run or as a Wherobots Notebook.\n",
    "\n",
    "This example discusses how to complete raster inference within a Wherobots Notebook. To complete this as a Job Run, the code samples referenced in subsequent sections would go into a single Python file and then be executed as a Job Run.\n",
    "For more information on creating Job Runs in Wherobots, see [WherobotsRunOperator](https://docs.wherobots.com/latest/develop/run-operator/).\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "Before attempting to use your own machine learning model in WherobotsAI Raster Inference, ensure that you have the following:\n",
    "\n",
    "* A Professional Edition Wherobots Organization.\n",
    "    * [Log in to Wherobots Cloud](https://cloud.wherobots.com) to follow along in an interactive Wherobots notebook or complete these steps for your own model in a new notebook.\n",
    "* A PyTorch model file.\n",
    "* An [Amazon S3 Bucket](https://docs.wherobots.com/latest/develop/storage-management/s3-storage-integration/) or [a Wherobots Managed Storage](https://docs.wherobots.com/latest/develop/storage-management/storage/#wherobots-managed-storage) for storing your MLM JSON file.\n",
    "\n",
    "## Save and Upload Your Model\n",
    "\n",
    "Save your model checkpoint using Torchscript. For more information, see [Saving and Loading Models](https://pytorch.org/tutorials/beginner/saving_loading_models.html) in the PyTorch documentation.\n",
    "\n",
    "The following Torchscript model checkpoint saving methods are supported:\n",
    "\n",
    "| Artifact Type | Description |\n",
    "| ----- | ----- |\n",
    "| `torch.jit.script` | A model artifact obtained by [`TorchScript`](https://pytorch.org/docs/stable/jit.html). |\n",
    "| `torch.export.save` | A Pytorch model archive containing an artifact of type [`AOTInductor`](https://docs.pytorch.org/tutorials/recipes/torch_export_aoti_python.html#when-to-use-aotinductor-with-a-python-runtime) or [`ExportedProgram`](https://docs.pytorch.org/docs/2.7/export.htm). |\n",
    "\n",
    "!!! note\n",
    "    WherobotsAI Raster Inference currently only supports PyTorch models.\n",
    "\n",
    "1. Store your model in an S3 bucket. This S3 bucket needs to be accessible to Wherobots Cloud. You can choose to store your model in one of two ways:\n",
    "    1. Directly in Wherobots Managed Storage. For more information, see [Wherobots storage and notebook guidance](https://docs.wherobots.com/latest/develop/storage-management/storage/#wherobots-notebook-and-data-storage-guidance).\n",
    "    2. Integrate your existing Amazon S3 storage with Wherobots. For more information on integrating a public or private S3 bucket with Wherobots Cloud see, [S3 storage integration](https://docs.wherobots.com/latest/develop/storage-management/s3-storage-integration/).\n",
    "\n",
    "In this example, we’ll store our model using [Wherobots Managed Storage](https://docs.wherobots.com/latest/develop/storage-management/storage/#wherobots-managed-storage) and create a `data/customer-XXXX/bring-your-own-model` directory.\n",
    "\n",
    "!!! note\n",
    "    This example uploads the model to Wherobots Managed Storage but you can also use your model through integrated storage. For more information, see [S3 storage integration](https://docs.wherobots.com/latest/develop/storage-management/s3-storage-integration/) in the Wherobots documentation.\n",
    "\n",
    "  ![Upload model](./assets/img/byom-model-pt.png)\n",
    "\n",
    "The URI to this model is used to create an MLM JSON in the subsequent step.\n",
    "\n",
    "## Create an MLM JSON for Your Model\n",
    "\n",
    "### MLM specification overview\n",
    "\n",
    "The [Machine Learning Model Extension Specification](https://github.com/stac-extensions/mlm) (MLM) is based on the [SpatioTemporal Asset Catalog](https://github.com/radiantearth/stac-spec)’s (STAC) standardized MLM. MLM defines a JSON format that specifies a\n",
    "model’s properties, input and input processing requirements, and output and output processing\n",
    "requirements.\n",
    "\n",
    "MLM creates a standardized way to use your own models for inference. MLM accomplishes this by:\n",
    "\n",
    "* Enabling the building of searchable custom models and their associated STAC datasets.\n",
    "* Recording all necessary bands, parameters, modeling artifact locations, and high-level processing steps to deploy an inference service.\n",
    "* Creating an easy and standardized way to use your own models for inference.\n",
    "\n",
    "### MLM specification forms\n",
    "\n",
    "To create an MLM JSON for your model, first fill out the\n",
    "Model Asset Form in the **Asset Form** tab and then fill out the Model Metadata form in the\n",
    "**MLM Form** tab.\n",
    "\n",
    "!!! info\n",
    "    You must fill out the **Asset Form** before the **MLM Form**.\n",
    "\n",
    "#### Fill out Asset Form\n",
    "\n",
    "To fill out the Model Asset Form, do the following:\n",
    "\n",
    "1. Go to the [Machine Learning Model Metadata Form](https://mlm-form.vercel.app/asset) site.\n",
    "1. Go to the [Asset Form](https://mlm-form.vercel.app/asset) tab.\n",
    "   ![Model asset form](assets/img/asset-form.png)\n",
    "1. Fill in the MLM Model Asset Form with your model information in accordance with the following chart. For compatibility with Raster Inference, you only need to specify the URI to the model artifact. For additional information and metadata fields you may want to document for your model, see [Model Asset](https://github.com/stac-extensions/mlm?tab=readme-ov-file#model-asset) in Machine Learning Model Extension Specification.\n",
    "\n",
    "    | Field Name | Type | Required or optional | Description |\n",
    "    | ----- | ----- | ----- | ----- |\n",
    "    | Title | string | Optional | Name of model asset |\n",
    "    | URI | string | Required | S3 URI to your saved Torchscript model. |\n",
    "#### Fill out MLM form\n",
    "\n",
    "To create the MLM JSON your model, do the following:\n",
    "\n",
    "1. Within the [Machine Learning Model Metadata Form](https://mlm-form.vercel.app/) site, go to the **MLM Form** tab.\n",
    "    ![MLM form](assets/img/mlm-form.png)\n",
    "    * This form validates your input formats so that they conform to the [MLM specification](https://github.com/stac-extensions/mlm). For clarity, we’ve specified a few fields for reference below. For a full breakdown of the inputs and definitions, see [Item Properties and Collection Fields](https://github.com/stac-extensions/mlm) in the Machine Learning Model Extension Specification.\n",
    "\n",
    "    | MLM metadata form field | Expected Input | Example Input |\n",
    "    | ----- | ----- | ----- |\n",
    "    | Is it pretrained? | true or false | true |\n",
    "    | Categories | List of classes for your model | “Solar panels”, “Wind farms”, “Forests” |\n",
    "\n",
    "1. Click **Download JSON** to save the JSON file.\n",
    "\n",
    "[Here is a reference MLM](https://huggingface.co/wherobots/mlm-stac/blob/2fd1a21026b0f80d4f7721605a3fc1f4ca389dfa/classification/landcover-eurosat-sentinel2/model-metadata.json#L134-L185) for the `landcover-eurostat-sentinel2` Wherobots hosted model.\n",
    "\n",
    "## Upload your model’s MLM JSON\n",
    "\n",
    "We created an MLM JSON for the Torchscript model by following the steps in [Create an MLM JSON for Your Model](#create-an-mlm-json-for-your-model).\n",
    "\n",
    "1. Upload the JSON to the same S3 bucket as the Torchscript model in Wherobots.\n",
    "\n",
    "    ![Upload json](assets/img/byom-storage.png)\n",
    "\n",
    "The path to the MLM JSON will be the `user_mlm_uri` in the rest of the example.\n",
    "\n",
    "## Run Inference Using Your Model on Raster Data\n",
    "\n",
    "Currently, WherobotsAI Raster Inference supports running model inference on the following tasks:\n",
    "\n",
    "* Single-label scene classification\n",
    "* Object Detection\n",
    "* Semantic Segmentation\n",
    "* Text to Bounding Boxes\n",
    "* Text to Instance Segments\n",
    "\n",
    "The following chart details the WherobotsAI Raster Inference function calls to use for each Computer Vision task.\n",
    "\n",
    "| Computer Vision Task | SQL API | Python API | Walk Through Example |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| Image Classification | `RS_Classify()` | `rs_classify()` | [Run inference for classification](https://docs.wherobots.com/latest/tutorials/wherobotsai/wherobots-inference/classification/?h=rs_classify) |\n",
    "| Object Detection | `RS_Detect_BBoxes()` | `rs_detect()` | [Run inference for Object Detection](https://docs.wherobots.com/latest/tutorials/wherobotsai/wherobots-inference/object_detection/) |\n",
    "| Semantic Segmentation | `RS_Segment()` , `RS_Segment_to_Geoms()` | `rs_segment()` | [Run inference for Semantic Segmentation](https://docs.wherobots.com/latest/tutorials/wherobotsai/wherobots-inference/segmentation/) |\n",
    "| Instance Segmentation | `RS_Text_To_Segments()` , | `rs_text_to_segments()` | [Run inference for Segment Anything 2](https://docs.wherobots.com/latest/tutorials/wherobotsai/wherobots-inference/raster-text-to-segments-airplanes/) |\n",
    "\n",
    "## Semantic Segmentation example\n",
    "\n",
    "In the following example, we’ll discuss how to use your own model for Raster Inference in\n",
    "Wherobots by performing Semantic Segmentation (also referred to as pixel classification) to identify solar farms in Arizona.\n",
    "\n",
    "This example uses:\n",
    "\n",
    "* A [Pytorch Archive](https://docs.pytorch.org/tutorials/recipes/torch_export_aoti_python.html#when-to-use-aotinductor-with-a-python-runtime) model fine-tuned from the [Satlas model](https://satlas.allen.ai/ai) <sup>1</sup> on Sentinel-2 multispectral satellite imagery to identify solar farms\n",
    "* An MLM JSON derived from the [Satlas model documentation](https://satlas.allen.ai/ai) for this task\n",
    "* A set of new Sentinel-2 multispectral satellite images sampled from the [Satlas dataset](https://satlas.allen.ai/ai)\n",
    "\n",
    "!!! note\n",
    "    This example is also available to walk through in `examples/Analyzing-Data/Bring_Your_Own_Model_Raster_Inference.ipynb` once you launch a Wherobots Notebook instance.\n",
    "\n",
    "To use your model for Semantic Segmentation, follow the steps in the subsequent sections to configure the MLM path, load the Torchscript model, and run Raster Inference on the new dataset.\n",
    "\n",
    "## Start a notebook\n",
    "\n",
    "To start a notebook to run raster inference with WherobotsAI, do the following:\n",
    "\n",
    "1. Log in to Wherobots Cloud.\n",
    "2. Start a Wherobots instance. We recommend using the **Tiny-GPU** runtime. It can take several minutes for a runtime to load.\n",
    "3. Open a Python notebook.\n",
    "    1. To interact with this example yourself, open `examples/Analyzing-Data/Bring_Your_Own_Model_Raster_Inference.ipynb`.\n",
    "    1. If you are incorporating you own model, create a new notebook. If using your own model, use the code samples in this tutorial as a guide.\n",
    "    !!! note\n",
    "        If you add the S3 storage integration after starting the notebook, you must restart the notebook in order to access to the newly added storage integration.\n",
    "\n",
    "For more information on starting a notebook, see [Notebook instance management](https://docs.wherobots.com/latest/develop/notebook-management/notebook-instance-management/)\n",
    "and [Jupyter Notebook Management](https://docs.wherobots.com/latest/develop/notebook-management/jupyter-notebook-management/).\n",
    "\n",
    "### Set Up The Sedona Context\n",
    "\n",
    "The following code creates the `SedonaContext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a610aea-71fc-45d1-95cf-b4fdd5598f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "config = (\n",
    "    SedonaContext.builder().appName('segmentation-batch-inference')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd57db-6b13-47cf-b90e-a25b60d5b2d5",
   "metadata": {},
   "source": [
    "### Create the URI variable\n",
    "\n",
    "Next, we need to set the `user_mlm_uri` path to the S3 URI of the MLM JSON that we created in [Upload your model's MLM JSON](#upload-your-models-mlm-json).\n",
    "\n",
    "WherobotsAI Raster Inference uses `user_mlm_uri` to get the necessary processing information\n",
    "for the model and know which model to use to run inference.\n",
    "\n",
    "To get the S3 URI of the MLM JSON:\n",
    "\n",
    "1. Navigate to the MLM JSON in Wherobots Cloud.\n",
    "1. Copy/paste the location of the file and set it to `user_mlm_uri`.\n",
    "\n",
    "![Copy MLM URI](assets/img/byom-storage-copy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d13d40-d40a-4b58-8969-ce90cebcc0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mlm_uri = [PATH-TO-YOUR-MLM-JSON]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd1fad6-64c3-4854-9c19-b57f16eebef7",
   "metadata": {},
   "source": [
    "### Load Satellite Imagery\n",
    "\n",
    "Load the satellite imagery that we will be running inference over. These GeoTiff images are\n",
    "loaded as [out-db rasters in WherobotsDB](https://docs.wherobots.com/latest/tutorials/wherobotsdb/raster-data/raster-load/), where each row represents a different scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b745f2-698d-4d05-94bb-a4b8eb2558a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_folder_path = 's3a://wherobots-benchmark-prod/data/ml/satlas/'\n",
    "df_raster_input = sedona.read.format(\"raster\").load(f\"{tif_folder_path}/*.tif\").sample(.05)\n",
    "df_raster_input.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95004810-4fd8-4a16-b1da-1fa36eec126f",
   "metadata": {},
   "source": [
    "### Run Predictions And Visualize Results\n",
    "\n",
    "#### Raster Inference SQL function RS_Segment\n",
    "\n",
    "To run predictions, specify the MLM model metadata file we saved to `user_mlm_uri`.\n",
    "\n",
    "Predictions can be run with this Raster Inference SQL function, [`RS_Segment`](https://docs.wherobots.com/latest/api/wherobots-inference/pythondoc/inference/sql_functions/#rs_segment) or the [Python API](#using-the-wherobotsinference-python-api).\n",
    "\n",
    "Here we generate 400 raster predictions using `RS_Segment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd2441-6230-49e6-894f-5b65ee104cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = sedona.sql(f\"\"\"\n",
    "SELECT\n",
    "  rast,\n",
    "  segment_result.*\n",
    "FROM (\n",
    "  SELECT\n",
    "    rast,\n",
    "    RS_SEGMENT('{user_mlm_uri}', rast) AS segment_result\n",
    "  FROM\n",
    "    df_raster_input\n",
    ") AS segment_fields\n",
    "\"\"\")\n",
    "\n",
    "predictions_df.cache().count()\n",
    "predictions_df.show()\n",
    "predictions_df.createOrReplaceTempView(\"predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee5769-9845-45cd-b75e-029fdbcf23e8",
   "metadata": {},
   "source": [
    "#### Using the wherobots.inference Python API\n",
    "\n",
    "For those who prefer working with Python, `wherobots.inference` provides a module to register\n",
    "SQL inference functions as Python functions.\n",
    "\n",
    "To use this module, replace the code in [Raster Inference SQL function RS_Segment](#raster-inference-sql-function-rs_segment) with the following code sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f8f7b-f121-450f-889c-4a1f535297d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wherobots.inference.engine.register import create_semantic_segmentation_udfs\n",
    "from pyspark.sql.functions import col\n",
    "rs_segment =  create_semantic_segmentation_udfs(batch_size = 9, sedona=sedona)\n",
    "df = df_raster_input.withColumn(\"segment_result\", rs_segment(user_mlm_uri, col(\"rast\"))).select(\n",
    "                               \"rast\",\n",
    "                               col(\"segment_result.confidence_array\").alias(\"confidence_array\"),\n",
    "                               col(\"segment_result.class_map\").alias(\"class_map\")\n",
    "                           )\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f82cb3-998a-4a7e-99a8-11eb766d56c3",
   "metadata": {},
   "source": [
    "### Extract insights\n",
    "\n",
    "#### Initial results\n",
    "\n",
    "Now that we've generated predictions using our model over our satellite imagery, we can use\n",
    "the `RS_Segment_To_Geoms` function to extract geometries from the classified imagery pixels.\n",
    "\n",
    "These geometries delineate the boundaries of possible solar farms and contain the average\n",
    "model confidence scores of the pixels contained within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef750f-8254-4fe8-aa4a-62d2baabdbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multipolys = sedona.sql(\"\"\"\n",
    "    WITH t AS (\n",
    "        SELECT RS_SEGMENT_TO_GEOMS(rast, confidence_array, array(1), class_map, 0.65) result\n",
    "        FROM predictions\n",
    "    )\n",
    "    SELECT result.* FROM t\n",
    "\"\"\")\n",
    "\n",
    "df_multipolys.cache().count()\n",
    "df_multipolys.show()\n",
    "df_multipolys.createOrReplaceTempView(\"multipolygon_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe296b5-4463-48c7-b9d8-c9764bfa9440",
   "metadata": {},
   "source": [
    "We'll specify the following:\n",
    "\n",
    "* `rast`: A raster column to use for georeferencing our results\n",
    "* `confidence_array`: The prediction result from the previous step\n",
    "* `array(1)`: Our category label \"1\" returned by the model representing Solar Farms\n",
    "* `class_map`: Class map to use for assigning labels to the prediction\n",
    "* `0.65`: A confidence threshold between 0 and 1 to use to threshold classified pixels from the model.\n",
    "\n",
    "#### Filtered results\n",
    "\n",
    "Since we ran inference across the *entire* state of Arizona, many scenes don't contain solar farms and as a result, don't have positive detections.\n",
    "\n",
    "Let's filter out scenes without segmentation detections so that we only retain the positive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979a4b0-5f9e-48db-bdb4-3a2b32e61b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_predictions = sedona.sql(\"\"\"\n",
    "    SELECT\n",
    "        element_at(class_name, 1) AS class_name,\n",
    "        cast(element_at(average_pixel_confidence_score, 1) AS double) AS average_pixel_confidence_score,\n",
    "        ST_Collect(geometry) AS merged_geom\n",
    "    FROM\n",
    "        multipolygon_predictions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe6918a-6496-4b73-93ea-8938500b3c98",
   "metadata": {},
   "source": [
    "This leaves us with a few predicted solar farm polygons for our 400 satellite image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69ccc1-9d76-4ad4-8c2d-4dc5258528c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_predictions = df_merged_predictions.filter(\"ST_IsEmpty(merged_geom) = False\")\n",
    "df_filtered_predictions.cache().count()\n",
    "df_filtered_predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff94e74-face-4c51-8aa1-2fb26794b914",
   "metadata": {},
   "source": [
    "#### Visualize with SedonaKepler\n",
    "\n",
    "We'll plot these filtered results with SedonaKepler. Compare the satellite basemap with the predictions and see if there's a match\\!\n",
    "\n",
    "!!! note\n",
    "    This basemap is compiled from images taken at different times. This means the features shown on the basemap might not match the imagery we just used for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa7b95-20d2-410c-bf36-cca1e3cc2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "config = {\n",
    "    'version': 'v1',\n",
    "    'config': {\n",
    "        'mapStyle': {\n",
    "            'styleType': 'dark',\n",
    "            'topLayerGroups': {},\n",
    "            'visibleLayerGroups': {},\n",
    "            'mapStyles': {}\n",
    "        },\n",
    "    }\n",
    "}\n",
    "map = SedonaKepler.create_map(config=config)\n",
    "\n",
    "SedonaKepler.add_df(map, df=df_filtered_predictions, name=\"Solar Farm Detections\")\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7012b168-01c2-4778-90a5-713b2a3a3386",
   "metadata": {},
   "source": [
    "1.  Bastani, Favyen, Wolters, Piper, Gupta, Ritwik, Ferdinando, Joe, and Kembhavi, Aniruddha. \"SatlasPretrain: A Large-Scale Dataset for Remote Sensing Image Understanding.\" *arXiv preprint arXiv:2211.15660* (2023). [https://doi.org/10.48550/arXiv.2211.15660](https://doi.org/10.48550/arXiv.2211.15660)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
