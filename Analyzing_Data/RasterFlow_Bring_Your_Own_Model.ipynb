{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3c78b8-6c84-4785-b91c-e879040c296d",
   "metadata": {},
   "source": [
    "# Bringing Your Own PyTorch Model to RasterFlow\n",
    "\n",
    "This guide shows how to export a PyTorch model, store it in S3, and run it in RasterFlow.\n",
    "\n",
    "We'll use a simple toy model with minimal dependencies to focus on the essential steps: **model export** and **RasterFlow integration**.\n",
    "\n",
    "---\n",
    "\n",
    "### Resources\n",
    "\n",
    "For exported model examples and scripts, see our [Hugging Face collection](https://huggingface.co/collections/wherobots/wherobotsai-models).\n",
    "\n",
    "**PyTorch PT2 export documentation:**\n",
    "- [Common Challenges and Solutions](https://docs.pytorch.org/tutorials/recipes/torch_export_challenges_solutions.html) — Beginner\n",
    "- [Export Tutorial](https://docs.pytorch.org/tutorials/intermediate/torch_export_tutorial.html) — Advanced (complex models, accelerator optimization)\n",
    "- [PT2 Archive Format](https://docs.pytorch.org/docs/stable/export/pt2_archive.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22059309-fee9-43ad-af79-567f88ba332d",
   "metadata": {},
   "source": [
    "## Create a Toy Model\n",
    "\n",
    "We'll create an example model matching the [Meta/WRI Canopy Height Model](https://huggingface.co/wherobots/meta-chm-v1-pt2) signature: the input is a tensor of image data, output is a tensor of continuous values (canopy height in meters). The shape of the output is the same as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79bdf4-e57c-4afa-8be6-a8e0ff540ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install torch==2.8 torchvision --extra-index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8016b00-8883-4685-b0e7-492006756250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and load checkpoint\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ExampleModel(nn.Module):\n",
    "    def forward(self, x):\n",
    "        predictions = torch.randn(x.shape)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e03ef-e14b-453e-b3d0-b952a4279044",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,3,256,256)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb86078-8883-4fff-afd1-b21f6b3419e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExampleModel()\n",
    "result = model(x)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fec5db-ebe5-4d2d-a6b4-959689d26318",
   "metadata": {},
   "source": [
    "With our model defined, let's now export it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05996394-1998-4b32-a24a-026f0c059a56",
   "metadata": {},
   "source": [
    "## PyTorch 2 Export Formats\n",
    "\n",
    "PyTorch offers multiple export formats for different use cases: storing weights, training, edge inference, and server inference.\n",
    "\n",
    "### Why not export with the `.pth` format?\n",
    "\n",
    "You may be familiar with the checkpoint format saved as `.pth`:\n",
    "\n",
    "```python\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "```\n",
    "\n",
    "This only stores model weights, not the model structure or execution logic. Loading requires all original dependencies:\n",
    "\n",
    "```python\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "```\n",
    "\n",
    "Additionally, `state_dict` exports can't be optimized for specific accelerators, making deployment difficult.\n",
    "\n",
    "### The `.pt2` Format — A Better Alternative\n",
    "\n",
    "`torch.export` produces a single artifact for both training and inference with some key benefits:\n",
    "\n",
    "1. **Device flexibility** — Export on CPU, load parameters to GPU at runtime\n",
    "2. **Accelerator optimization** — Compile for faster execution on CUDA, AMD, or Intel GPUs\n",
    "3. **Standardized metadata** — Store hyperparameters, configuration, and transforms alongside the model\n",
    "\n",
    "---\n",
    "\n",
    "We'll export our `ExampleModel` in PT2 format. For input preprocessing, we'll export transforms as an `nn.Module` in the same archive. Using `torch.nn.Sequential` simplifies export since its `forward` method takes a single `input` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36dee7-0f24-487d-97ef-d13fb440b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torchvision.transforms.v2 as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e146e-4efc-41d3-89cc-15a2c452b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_band_mean= [.5]*len(x)\n",
    "per_band_std = [.1]*len(x)\n",
    "norm_transform = torch.nn.Sequential(T.Normalize(mean=per_band_mean, std=per_band_std))\n",
    "print(isinstance(norm_transform, nn.Module))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff5ce9-c44f-4167-a41c-656b07871a53",
   "metadata": {},
   "source": [
    "## Defining Input Shape Constraints\n",
    "\n",
    "`torch.export` needs to know the expected input shape. Use `Dim` objects to specify:\n",
    "- **Dynamic** dimensions — can be any value ≥1\n",
    "- **Static** dimensions — must be a fixed size\n",
    "\n",
    "#### FAQ\n",
    "\n",
    "| Question | Answer |\n",
    "|----------|--------|\n",
    "| How do I know what shape to use? | Input shape affects runtime performance and accuracy. Check the model creator's recommendations. |\n",
    "| Should I use dynamic for all dimensions? | Typically no. Some models have data dependent control flow logic that requires fixed dimensions (e.g., object detection models). |\n",
    "\n",
    "**Rule of thumb:** Keep batch size dynamic; fix channel, height, and width. Test other dynamic configurations through trial and error to enable flexible channels, height, or width.\n",
    "\n",
    "We'll export with: **dynamic batch size** (Denoted by `Dim.Auto`), **3 channels**, **256×256 height/width**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a83526-f1e5-41db-81f6-3905584da759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.export import Dim\n",
    "input_shape_constraints = [-1, 3, 256, 256]\n",
    "example_input_shape = [2, 3, 256, 256]\n",
    "example_tensor = torch.randn(*example_input_shape, requires_grad=False)\n",
    "dims = tuple(Dim.AUTO if dim == -1 else dim for dim in input_shape_constraints)\n",
    "print(dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed8352-e7c0-42ed-a13c-7a151703aeef",
   "metadata": {},
   "source": [
    "Set the model to `eval` mode and configure device/dtype before export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee11e4-1ffc-4440-a4f1-2b6b33ed5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "device = torch.device(\"cuda\")\n",
    "dtype = torch.float32\n",
    "model.eval()\n",
    "model = model.to(device).to(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da07463-a44c-4191-a302-d07b9f054a50",
   "metadata": {},
   "source": [
    "`torch.export` needs the forward function's argument names. Parse them using Python's `inspect` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a9869-7892-4033-b2cf-2961927647e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arg = next(iter(inspect.signature(model.forward).parameters))\n",
    "print(model_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6493a990-19e6-4f7e-9337-27e8d715c2bb",
   "metadata": {},
   "source": [
    "## Export to ExportedProgram\n",
    "\n",
    "Now that we have the following inputs we can export to an `ExportedProgram`, an in-memory model object that can be saved to a `.pt2` archive.\n",
    "\n",
    "* `model`, we defined this toy model earlier as an nn.Module\n",
    "* `args`, a tuple of the arguments to the model's forward pass function\n",
    "* `dynamic_shapes`, a dict mapping the argument name of the input to the tuple of dimension constraints we created earlier: `dims`\n",
    "\n",
    "The `ExportedProgram` includes the `state_dict` (weights) and `example_inputs` (useful for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a1cd4b-286c-465f-98b8-f0e1c8fc5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_program = torch.export.export(mod=model, args=(example_tensor,), dynamic_shapes={model_arg: dims})\n",
    "print(model_program.example_inputs[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d66be2-540a-4d7b-84ae-825f2cd6c8bc",
   "metadata": {},
   "source": [
    "Follow the same export steps for the transforms module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57054f33-af49-4e92-adea-34f6e06c1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_transform.eval()\n",
    "norm_transform = norm_transform.to(device).to(dtype)\n",
    "transform_arg = next(iter(inspect.signature(norm_transform.forward).parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887e495-c4fe-4e41-b335-4888956c2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_program = torch.export.export(\n",
    "    mod=norm_transform, args=(example_tensor,), dynamic_shapes={transform_arg: dims}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378be3c7-f471-4541-a94c-a9c6d3e4f6e9",
   "metadata": {},
   "source": [
    "## Save to `.pt2` Archive\n",
    "\n",
    "> **Note:** `torch.export.save` saves a single ExportedProgram. We'll use `torch.export.pt2_archive._package.package_pt2` to bundle both the model and transforms into one `.pt2` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d113f364-63d4-4d26-bf51-e02da1b59002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.export.pt2_archive._package import package_pt2\n",
    "exported_programs = {}\n",
    "local_model_path = \"example.pt2\"\n",
    "exported_programs[\"model\"] = model_program\n",
    "exported_programs[\"transforms\"] = transforms_program\n",
    "\n",
    "package_pt2(\n",
    "    f=local_model_path,\n",
    "    exported_programs=exported_programs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d5a14d-3199-4484-8529-1697d63b3acc",
   "metadata": {},
   "source": [
    "## Running the Custom Model with RasterFlow\n",
    "\n",
    "RasterFlow supports both Wherobots Hosted Models and custom models like the one we just exported.\n",
    "\n",
    "**Steps:**\n",
    "1. Upload the `.pt2` file to S3 (we'll use Wherobots Managed Storage)\n",
    "2. Define an `InferenceConfig` to tell RasterFlow how to run the model\n",
    "\n",
    "Note: You can also load open models stored in pt2 format directly from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7cb39-3ab4-4b44-8d5c-3db7b5a379a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(profile=\"default\")\n",
    "\n",
    "# Define the destination path on S3\n",
    "# We use the USER_S3_PATH environment variable to ensure it goes to your personal bucket space\n",
    "s3_model_path = os.getenv(\"USER_S3_PATH\") + local_model_path\n",
    "fs.put(local_model_path, s3_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86e159-7b45-47fc-b287-d2daea6a4c5a",
   "metadata": {},
   "source": [
    "## Build the Model Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574be63-62d5-4314-9b8e-21de48f0e89a",
   "metadata": {},
   "source": [
    "To run our model, we need some input imagery. We'll test our model on Sentinel-2 4 band imagery - red, blue, greee, near infrared. We'll select an AOI over Nashua, New Jersey that has some forest canopy for our toy canopy height model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b96a00-32eb-4840-b63d-7a125990e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wkls\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "# Generate a geometry for Nashua, NH using WKLS (https://github.com/wherobots/wkls)\n",
    "gdf = gpd.read_file(wkls.us.nh.nashua.geojson())\n",
    "\n",
    "# Save the geometry to a parquet file in the user's S3 path\n",
    "aoi_path = os.getenv(\"USER_S3_PATH\") + \"nashua.parquet\"\n",
    "gdf.to_parquet(aoi_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83748dff-fe14-4580-8e51-82d56f5e7cf3",
   "metadata": {},
   "source": [
    "To prepare this imagery, we'll use RasterFlow to create a mosaic. Mosaics are backed by a cloud native Zarr store that enables accessing spatial subsets, individual bands, and computing on the mosaic with RasterFlow.\n",
    "\n",
    "This workflow takes a few minutes to complete, so you can skip ahead to the next cell where we load the prepared output from the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9984a0-ab8e-4214-bb62-7588968c5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterflow_remote import RasterflowClient\n",
    "client = RasterflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae57e6-8a90-4d79-81bb-2f2f1fd58fc0",
   "metadata": {},
   "source": [
    "```python\n",
    "mosaic_path = client.build_gti_mosaic(\n",
    "        gti = \"s3://wherobots-examples/rasterflow/indexes/naip_index.parquet\",\n",
    "        aoi = aoi_path,\n",
    "        bands = [\"red\", \"green\", \"blue\", \"nir\"],\n",
    "        location_field = \"url\",\n",
    "        crs_epsg = 3857,\n",
    "        xy_chunksize = 1024,\n",
    "        query = \"res == .6\",\n",
    "        requester_pays = True,\n",
    "        sort_field = \"time\",\n",
    "        resampling = ResamplingMethod.NEAREST,\n",
    "        nodata= 0.0,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f38e83-1968-42b5-8987-dcc815281c45",
   "metadata": {},
   "source": [
    "## Visualize a subset of the model outputs\n",
    "We will use hvplot and datashader to visualize a small subset of the mosaic's red band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf251f9-2d95-4061-9ef4-18299321ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for visualization and coordinate transformation\n",
    "import hvplot.xarray\n",
    "import xarray as xr\n",
    "import s3fs \n",
    "import zarr\n",
    "from pyproj import Transformer\n",
    "from holoviews.element.tiles import EsriImagery \n",
    "\n",
    "# Open the Zarr store\n",
    "mosaic_path = \"s3://wherobots-examples/rasterflow/mosaics/nashua.zarr\"\n",
    "fs = s3fs.S3FileSystem(profile=\"default\", asynchronous=True, anon=True)\n",
    "zstore = zarr.storage.FsspecStore(fs, path=mosaic_path)\n",
    "ds = xr.open_zarr(zstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e965e9f7-6eee-4472-b01a-a4744994cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer to convert from lat/lon to meters\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3857\", always_xy=True)\n",
    "\n",
    "# Transform bounding box coordinates from lat/lon to meters\n",
    "min_lon, min_lat, max_lon, max_lat = gdf.total_bounds\n",
    "(min_x, max_x), (min_y, max_y) = transformer.transform(\n",
    "    [min_lon, max_lon], \n",
    "    [min_lat, max_lat]\n",
    ")\n",
    "\n",
    "# Select the red band and slice the dataset to the bounding box\n",
    "# y=slice(max_y, min_y) handles the standard \"North-to-South\" image orientation\n",
    "ds_subset = ds.sel(band=\"red\",\n",
    "    x=slice(min_x, max_x), \n",
    "    y=slice(max_y, min_y) \n",
    ")\n",
    "\n",
    "# Select the first time step and extract the variables array\n",
    "arr_subset = ds_subset.isel(time=0)[\"variables\"]\n",
    "\n",
    "# Create a base map layer using Esri satellite imagery\n",
    "base_map = EsriImagery()\n",
    "\n",
    "# Create an overlay layer from the model outputs with hvplot\n",
    "output_layer = arr_subset.hvplot(\n",
    "    x = \"x\",\n",
    "    y = \"y\",\n",
    "    geo = True,           # Enable geographic plotting\n",
    "    dynamic = True,       # Enable dynamic rendering for interactivity\n",
    "    rasterize = True,     # Use datashader for efficient rendering of large datasets\n",
    "    cmap = \"viridis\",     # Color map for visualization\n",
    "    aspect = \"equal\",     # Maintain equal aspect ratio\n",
    "    title = \"Nashua, NJ Sentinel-2 Red Band\" \n",
    ").opts(\n",
    "    width = 600, \n",
    "    height = 600,\n",
    "    alpha = 0.7           # Set transparency to see the base map underneath\n",
    ")\n",
    "\n",
    "# Combine the base map and output layer\n",
    "final_plot = base_map * output_layer\n",
    "final_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2cc311-3b60-49de-9aa7-649e92f25e21",
   "metadata": {},
   "source": [
    "With our mosaic, we are now ready to run model prediction on the mosaic with RasterFlow.\n",
    "\n",
    "We'll use the [`predict_mosaic`](https://docs.wherobots.com/reference/rasterflow/client#predict_mosaic) method to run our model. `predict_mosaic` leverages RasterFlow's powerful inference engine that scales from small to global scale areas of interest.\n",
    "\n",
    "The inputs to this method are our input store we want to run prediction on, and our InferenceConfig object we created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6d744-beaa-47fa-b53b-22f7518c965e",
   "metadata": {},
   "source": [
    "## Defining the InferenceConfig\n",
    "\n",
    "With the model on S3, define the inference job configuration.\n",
    "\n",
    "> **Note:** Wherobots Hosted Models come with preconfigured `ModelRecipes`— this step is only needed for custom models.\n",
    "\n",
    "See the [InferenceConfig documentation](https://docs.wherobots.com/reference/RasterFlow/data-models#inferenceconfig) for parameter details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c520ebc-6aa6-4e83-beb9-903287cbb987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "from rasterflow_remote.data_models import InferenceConfig, InferenceActorEnum, MergeModeEnum, ResamplingMethod\n",
    "\n",
    "custom_inference_config = InferenceConfig(\n",
    "    model_path = s3_model_path,\n",
    "    actor = InferenceActorEnum.REGRESSION_PYTORCH,\n",
    "    patch_size = 224,\n",
    "    clip_size = 28,\n",
    "    device = \"cuda\",\n",
    "    features = [\"red\", \"green\", \"blue\"],\n",
    "    labels = [\"canopy_height\"],\n",
    "    max_batch_size=64,\n",
    "    merge_mode = MergeModeEnum.WEIGHTED_AVERAGE\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1a023b-06bc-440d-8f72-97da438ac997",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.predict_mosaic(\n",
    "        store=mosaic_path,\n",
    "        **asdict(custom_inference_config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb1fce-f486-4b61-b6c3-82f037e36469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
