{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa47acec",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/chm-banner.jpg\">\n",
    "\n",
    "# Estimating canopy height with RasterFlow\n",
    "\n",
    "This notebook will guide you through estimating canopy height from aerial imagery, using Wherobots RasterFlow and the Canopy Height Model. You will gain a hands-on understanding of how to run models like the Meta and World Resource Institute's Canopy Height prediction model on your selected area of interest.\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "This notebook will teach you to:\n",
    "\n",
    "* Use RasterFlow to run GeoAI models like the Meta and World Resource Institute's Canopy Height prediction model and generate inference results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a424ebf",
   "metadata": {},
   "source": [
    "## Wherobots RasterFlow\n",
    "\n",
    "With Wherobots RasterFlow, developers can use GeoAI models to generate insights at scale from satellite and aerial imagery.\n",
    "\n",
    "RasterFlow has built-in open models for common use-cases, including the Canopy Height Model (CHM) for predicting the height of the tree canopy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76561f61",
   "metadata": {},
   "source": [
    "### Meta and World Resource Institute's Canopy Height prediction model (Meta CHM v1)\n",
    "\n",
    "The [Meta and World Resource Institute's Canopy Height prediction model (Meta CHM v1)](https://github.com/facebookresearch/HighResCanopyHeight/)<sup>1</sup> model is an example of an open regression model that can predict the height of the tree canopy from high resolution imagery.\n",
    "\n",
    "This model generates a raster where each pixel is the estimated tree canopy height.\n",
    "\n",
    "It was trained on high-resoluton imagery ([Maxar Vivid2 mosaic](https://pro-docs.maxar.com/en-us/VividMosaics/VividMosaics_intro.htm) with 0.5m resolution), existing labeled canopy height maps and airborne laser scans (LIDAR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a65529",
   "metadata": {},
   "source": [
    "## Selecting an Area of Interest (AOI)\n",
    "We will choose an Area of Interest (AOI) for our analysis. The area around Nashua, NH has a combination of urban settings, parks and forests so we will try out the model there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7139d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wkls\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "# Generate a geometry for Nashua, NH using WKLS (https://github.com/wherobots/wkls)\n",
    "gdf = gpd.read_file(wkls.us.nh.nashua.geojson())\n",
    "\n",
    "# Save the geometry to a parquet file in the user's S3 path\n",
    "aoi_path = os.getenv(\"USER_S3_PATH\") + \"nashua.parquet\"\n",
    "gdf.to_parquet(aoi_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2b94c",
   "metadata": {},
   "source": [
    "## Initializing the RasterFlow client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4534b-5ca6-448f-8215-c4a97c410452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from rasterflow_remote import RasterflowClient\n",
    "\n",
    "from rasterflow_remote.data_models import (\n",
    "    ModelRecipes\n",
    ")\n",
    "\n",
    "rf_client = RasterflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e414dee",
   "metadata": {},
   "source": [
    "## Running a model\n",
    "\n",
    "RasterFlow has pre-defined workflows to simplify orchestration of the processing steps for model inference.  These steps include:\n",
    "* Ingesting imagery for the specified Area of Interest (AOI)\n",
    "* Generating a seamless image from multiple image tiles (a mosaic) \n",
    "* Running inference with the selected model\n",
    "\n",
    "The output is a Zarr store of the model outputs.\n",
    "\n",
    "Note: This step will take approximately 20 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = rf_client.build_and_predict_mosaic_recipe(\n",
    "    # Path to our AOI in GeoParquet or GeoJSON format\n",
    "    aoi = aoi_path,\n",
    "\n",
    "    # Date range for imagery to be used by the model\n",
    "    start = datetime(2023, 1, 1),\n",
    "    end = datetime(2024, 1, 1),\n",
    "\n",
    "    # Coordinate Reference System EPSG code for the output mosaic   \n",
    "    crs_epsg = 3857,\n",
    "\n",
    "    # The model recipe to be used for inference (FTW in this case)\n",
    "    model_recipe = ModelRecipes.META_CHM_V1\n",
    ")\n",
    "\n",
    "print(model_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fc2ab",
   "metadata": {},
   "source": [
    "## Visualize a subset of the model outputs\n",
    "We will use hvplot and datashader to visualize a small subset of the model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1778a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for visualization and coordinate transformation\n",
    "import hvplot.xarray\n",
    "import xarray as xr\n",
    "import s3fs \n",
    "import zarr\n",
    "from pyproj import CRS, Transformer\n",
    "from holoviews.element.tiles import EsriImagery \n",
    "\n",
    "# Select the field_boundaries class from the model outputs to visualize\n",
    "class_to_visualize = \"height\"\n",
    "\n",
    "# Open the Zarr store and select the field_boundaries band\n",
    "fs = s3fs.S3FileSystem(profile = \"default\", asynchronous=True)\n",
    "zstore = zarr.storage.FsspecStore(fs, path=model_outputs[5:])\n",
    "ds = xr.open_zarr(zstore).sel(band=class_to_visualize)\n",
    "\n",
    "# Define the bounding box coordinates for an area in Nashua, NH\n",
    "min_lon = -71.56  \n",
    "max_lon = -71.54   \n",
    "min_lat = 42.77  \n",
    "max_lat = 42.79  \n",
    "\n",
    "# Set up coordinate reference systems for transformation\n",
    "source_crs = CRS.from_epsg(4326)  # WGS84 (lat/lon)\n",
    "target_crs = CRS.from_epsg(3857)  # Web Mercator (meters)\n",
    "\n",
    "# Create a transformer to convert from lat/lon to meters\n",
    "transformer = Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "\n",
    "# Transform the bounding box coordinates from lat/lon to meters\n",
    "x_coords = [min_lon, max_lon]\n",
    "y_coords = [min_lat, max_lat]\n",
    "\n",
    "x_meters, y_meters = transformer.transform(x_coords, y_coords)\n",
    "\n",
    "# Extract the min/max values for slicing the dataset\n",
    "x_slice_min = min(x_meters)\n",
    "x_slice_max = max(x_meters)\n",
    "y_slice_min = min(y_meters)\n",
    "y_slice_max = max(y_meters)\n",
    "\n",
    "# Slice the dataset to the bounding box (note: y is reversed for correct orientation)\n",
    "ds_subset = ds.sel(\n",
    "    x=slice(x_slice_min, x_slice_max),\n",
    "    y=slice(y_slice_max, y_slice_min) \n",
    ")\n",
    "\n",
    "# Select the first time step and extract the variables array\n",
    "arr_subset = ds_subset.isel(time=0)[\"variables\"]\n",
    "\n",
    "# Create a base map layer using Esri satellite imagery\n",
    "base_map = EsriImagery()\n",
    "\n",
    "# Create an overlay layer from the model outputs with hvplot\n",
    "output_layer = arr_subset.hvplot(\n",
    "    x = \"x\",\n",
    "    y = \"y\",\n",
    "    geo = True,           # Enable geographic plotting\n",
    "    dynamic = True,       # Enable dynamic rendering for interactivity\n",
    "    rasterize = True,     # Use datashader for efficient rendering of large datasets\n",
    "    cmap = \"viridis\",     # Color map for visualization\n",
    "    aspect = \"equal\",     # Maintain equal aspect ratio\n",
    "    title = \"CHM Model Outputs\" \n",
    ").opts(\n",
    "    width = 600, \n",
    "    height = 600,\n",
    "    alpha = 0.7           # Set transparency to see the base map underneath\n",
    ")\n",
    "\n",
    "# Combine the base map and output layer\n",
    "final_plot = base_map * output_layer\n",
    "final_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e79110",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. **Tolan, J., Yang, H.-I., Nosarzewski, B., Couairon, G., Vo, H. V., Brandt, J., Spore, J., Majumdar, S., Haziza, D., Vamaraju, J., et al. (2024).** Very high resolution canopy height maps from RGB imagery using self-supervised vision transformer and convolutional decoder trained on aerial lidar. *Remote Sensing of Environment*, 300, 113888."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
