{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa47acec",
   "metadata": {},
   "source": [
    "# Change Detection with RasterFlow\n",
    "\n",
    "This notebook demonstrates RasterFlow's change detection inference workflow using the [Fields of the World (FTW)](https://fieldsofthe.world/)<sup>1</sup> model as an example.\n",
    "\n",
    "**Related notebooks:**\n",
    "- [RasterFlow_FTW.ipynb](RasterFlow_FTW.ipynb) — Complete FTW field boundary detection tutorial\n",
    "- [RasterFlow_Bring_Your_Own_Model.ipynb](RasterFlow_Bring_Your_Own_Model.ipynb) — Export custom PyTorch models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76561f61",
   "metadata": {},
   "source": [
    "### Why Fields of the World (FTW)?\n",
    "\n",
    "FTW predicts 3 classes (`non_field_background`, `field`, `field_boundaries`). While FTW is a field boundary segmentation model (not a true change detection model), it takes bi-temporal input (two seasons), making it useful for demonstrating change detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65b866-29db-401e-8e53-65a673d46e88",
   "metadata": {},
   "source": [
    "### Model signature\n",
    "\n",
    "Change detection models receive multi-temporal imagery stacked along the channel dimension:\n",
    "\n",
    "| | Shape |\n",
    "|---|---|\n",
    "| **Input** | `(Batch, Channels × TimeSteps, H, W)` — e.g. `(N, 8, 256, 256)` for 4 bands × 2 seasons |\n",
    "| **Output** | `(Batch, Classes, H, W)` — e.g. `(N, 3, 256, 256)` for 3 classes |\n",
    "\n",
    "```python\n",
    "class ChangeDetectionModel(nn.Module):\n",
    "    def forward(self, x):  # x: (N, Channels*TimeSteps, H, W)\n",
    "        return logits      # (N, Classes, H, W)\n",
    "```\n",
    "\n",
    "We'll use a particular **Actor** to run change detection inference using Rasterflow: `InferenceActorEnum.SEMANTIC_SEGMENTATION_CHANGE_DETECTION_PYTORCH`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a65529",
   "metadata": {},
   "source": [
    "## Selecting an Area of Interest (AOI)\n",
    "To start, we will choose an Area of Interest (AOI) for our analysis. The area around Haskell County, Kansas has some interesting crop field patterns so we will try out the model there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7139d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wkls\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "# Generate a geometry for Haskell County, Kansas using WKLS (https://github.com/wherobots/wkls)\n",
    "gdf = gpd.read_file(wkls['us']['ks']['Haskell County'].geojson())\n",
    "\n",
    "# Save the geometry to a parquet file in the user's S3 path\n",
    "aoi_path = os.getenv(\"USER_S3_PATH\") + \"haskell.parquet\"\n",
    "gdf.to_parquet(aoi_path)\n",
    "\n",
    "# Make variables for the bounds of our aoi for visualizing results after inference\n",
    "min_lon, min_lat, max_lon, max_lat = gdf.total_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2b94c",
   "metadata": {},
   "source": [
    "## Initializing the RasterFlow client\n",
    "\n",
    "See our docs to learn more about all methods available on the client https://docs.wherobots.com/reference/rasterflow/client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4534b-5ca6-448f-8215-c4a97c410452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from rasterflow_remote import RasterflowClient, DatasetEnum, InferenceActorEnum\n",
    "\n",
    "from rasterflow_remote.data_models import (\n",
    "    ModelRecipes, \n",
    "    VectorizeMethodEnum,\n",
    "    MergeModeEnum\n",
    ")\n",
    "\n",
    "rf_client = RasterflowClient(mosaics_version=\"v0.21.1\", rasterflow_version=\"v1.45.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e414dee",
   "metadata": {},
   "source": [
    "## Building a Mosaic\n",
    "\n",
    "RasterFlow provides a `build_mosaic` workflow to create analysis-ready imagery for your Area of Interest (AOI). This step:\n",
    "* Ingests Sentinel-2 imagery for the specified AOI across your defined time range (e.g., 2 years)\n",
    "* Applies quality filtering and cloud masking to select valid observations\n",
    "* Generates a seamless, temporally-composited mosaic from multiple image tiles\n",
    "\n",
    "The output is a Zarr store containing the mosaic, ready for inference or analysis.\n",
    "\n",
    "This step takes about 5 minutes for this AOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbaa1a-d94c-49f8-89cb-38697c570d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = rf_client.build_mosaic(\n",
    "    datasets=[DatasetEnum.S2_MED_WINDOWED_PIXEL],\n",
    "    aoi=aoi_path,\n",
    "    start=datetime(2023, 1, 1),\n",
    "    end=datetime(2025, 1, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838789bb-173c-47d1-a7e9-b9314ee59db3",
   "metadata": {},
   "source": [
    "With `xarray` we can print out a nice representation of our mosaic and see that it has two time steps for our change detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077d3e0-a2bf-425f-9d66-c5d9de5115d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "mosaic = xr.open_zarr(store)\n",
    "\n",
    "mosaic['variables']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf055d-f664-4a15-a314-0db38d6d3e45",
   "metadata": {},
   "source": [
    "We can also see what bands are included in the mosaic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9ad3d-c731-42fc-9561-041ad7347217",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic['variables']['band']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeeefd9-e438-4221-b488-bbdc273937d7",
   "metadata": {},
   "source": [
    "## Mosaic Inference\n",
    "\n",
    "Now that we have built the mosaic and understand the features that were included, we can run `predict_mosaic`.\n",
    "\n",
    "We'll pass in the band names we want to predict, as well as other inference configs that specify how to run inference.\n",
    "\n",
    "- `store` is the mosaic we just created for sentinel-2\n",
    "- `model_path` can be a path to a Pytorch 2 Archive file on s3 or Huggingface. In this case we'll use the Fields of the World model from the Wherobots' Huggingface Collection.\n",
    "- `patch_size` controls the XY size of the array input to the model\n",
    "- `clip_size` in conjunction with `MergeModeEnum`, controls how to run overlapping windowd inference to reduce edge effects relative to non-overlapping inference\n",
    "- `device` specifies what device to use for running inference. Our runtimes are GPU only at this time, but you could run on the CPU device if you wanted to!\n",
    "- `features` selects what bands and in what order to run inference on\n",
    "- `labels` sets the labels on the models output. Order matters here base don what your model returns!\n",
    "- `actor` determines the kind of inference handler to use. The correct actor to select depends on the task your model is solving and the kind of input it expects. For change detection, we support models that take in an input where time and channels dimensions are stacked together. So the channel dimension is ordered like so in in this example:\n",
    "\n",
    "```\n",
    "        \"<time_1>s2med_windowed_pixel:B04\",\n",
    "        \"<time_1>s2med_windowed_pixel:B03\",\n",
    "        \"<time_1>s2med_windowed_pixel:B02\",\n",
    "        \"<time_1>s2med_windowed_pixel:B08\",\n",
    "        \"<time_2>s2med_windowed_pixel:B04\",\n",
    "        \"<time_2>s2med_windowed_pixel:B03\",\n",
    "        \"<time_2>s2med_windowed_pixel:B02\",\n",
    "        \"<time_2>s2med_windowed_pixel:B08\"\n",
    "```\n",
    "- `max_batch_size`: Maximum number of patches fed through the model in a single forward pass.\n",
    "    - Higher values = better GPU utilization but more GPU memory required\n",
    "    - Lower values = less memory but potentially slower inference\n",
    "- `xy_block_multiplier`: Controls how many Zarr chunks are processed together as a single block during inference. \n",
    "    - Higher values = larger blocks = fewer tasks but more memory per task\n",
    "    - Lower values = smaller blocks = more tasks but less memory per task\n",
    "    Use a smaller multiplier (e.g., 1) if you're running into memory issues; use the default (4) for better throughput when memory\n",
    "allows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = rf_client.predict_mosaic(\n",
    "    store=store,\n",
    "    model_path=\"https://huggingface.co/wherobots/ftw-v1.1-pt2/resolve/main/ftw-v1.1-ep.pt2\",\n",
    "    patch_size=256,\n",
    "    clip_size=32,\n",
    "    device=\"cuda\",\n",
    "    features=[\n",
    "        \"s2med_windowed_pixel:B04\",\n",
    "        \"s2med_windowed_pixel:B03\",\n",
    "        \"s2med_windowed_pixel:B02\",\n",
    "        \"s2med_windowed_pixel:B08\",\n",
    "    ],\n",
    "    labels=[\n",
    "        \"non_field_background\",\n",
    "        \"field\",\n",
    "        \"field_boundaries\",\n",
    "    ],\n",
    "    actor=InferenceActorEnum.SEMANTIC_SEGMENTATION_CHANGE_DETECTION_PYTORCH,\n",
    "    max_batch_size=128,\n",
    "    merge_mode=MergeModeEnum.WEIGHTED_AVERAGE,\n",
    "    xy_block_multiplier=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d9c00-9eaa-4c9f-864a-afe766120622",
   "metadata": {},
   "source": [
    "Let's check out our mosaic result with xarray to see what a Zarr change detection store looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f84df-1ecc-4c8d-bf24-b94e0f832896",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mosaic = xr.open_zarr(model_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b048d-50ac-4618-97ae-13f0381d737f",
   "metadata": {},
   "source": [
    "The prediction result mosaic is shaped similar to the input, height and width are the same.\n",
    "\n",
    "Rather than have 4 bands, it has 3 bands for the prediction result, representing the categories. The FTW model outputs 3 categories, whereas other models may output a binary classification and there will be only one prediction band.\n",
    "\n",
    "Instead of 2 time steps, there is only 1, representing the change interval. We index the `time` coordinate of the Zarr store using the start date used to build the input mosaic. We also store a time delta coordinate to represent the interval of time between the start_date and end_date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26abef66-86ce-4553-a0ba-373a94fb2929",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mosaic['variables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92666fc2-7bd2-479c-b17b-9dc92218b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = result_mosaic['variables'].coords['time'].values[0]\n",
    "print(\"start time:\", start_time)\n",
    "end_time = (result_mosaic['variables'].coords['time'] + result_mosaic['variables'].coords['time_delta']).values[0]\n",
    "print(\"end time\", end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fc2ab",
   "metadata": {},
   "source": [
    "## Visualize a subset of the model outputs\n",
    "The raster outputs from the model for this AOI are approximately 1GB.  We can choose a small subset of the data around the Plymell, Kansas and use hvplot to visualize the model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263eed28-9e56-49e7-ba42-9426aff0c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for visualization and coordinate transformation\n",
    "import hvplot.xarray\n",
    "import xarray as xr\n",
    "import s3fs \n",
    "import zarr\n",
    "from pyproj import Transformer\n",
    "from holoviews.element.tiles import EsriImagery \n",
    "\n",
    "# Open the Zarr store\n",
    "fs = s3fs.S3FileSystem(profile=\"default\", asynchronous=True)\n",
    "zstore = zarr.storage.FsspecStore(fs, path=model_outputs[5:])\n",
    "ds = xr.open_zarr(zstore)\n",
    "\n",
    "# Create a transformer to convert from lat/lon to meters\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3857\", always_xy=True)\n",
    "\n",
    "# Transform bounding box coordinates from lat/lon to meters\n",
    "(min_x, max_x), (min_y, max_y) = transformer.transform(\n",
    "    [min_lon, max_lon], \n",
    "    [min_lat, max_lat]\n",
    ")\n",
    "\n",
    "# Select the height variable and slice the dataset to the bounding box\n",
    "# y=slice(max_y, min_y) handles the standard \"North-to-South\" image orientation\n",
    "ds_subset = ds.sel(band=\"field_boundaries\",\n",
    "    x=slice(min_x, max_x), \n",
    "    y=slice(max_y, min_y) \n",
    ")\n",
    "\n",
    "# Select the first time step and extract the variables array\n",
    "arr_subset = ds_subset.isel(time=0)[\"variables\"]\n",
    "\n",
    "# Create a base map layer using Esri satellite imagery\n",
    "base_map = EsriImagery()\n",
    "\n",
    "# Create an overlay layer from the model outputs with hvplot\n",
    "output_layer = arr_subset.hvplot(\n",
    "    x = \"x\",\n",
    "    y = \"y\",\n",
    "    geo = True,           # Enable geographic plotting\n",
    "    dynamic = True,       # Enable dynamic rendering for interactivity\n",
    "    rasterize = True,     # Use datashader for efficient rendering of large datasets\n",
    "    cmap = \"viridis\",     # Color map for visualization\n",
    "    aspect = \"equal\",     # Maintain equal aspect ratio\n",
    "    title = \"FTW Model Outputs\" \n",
    ").opts(\n",
    "    width = 600, \n",
    "    height = 600,\n",
    "    alpha = 0.7           # Set transparency to see the base map underneath\n",
    ")\n",
    "\n",
    "# Combine the base map and output layer\n",
    "final_plot = base_map * output_layer\n",
    "final_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4106b",
   "metadata": {},
   "source": [
    "## Vectorize the raster model outputs\n",
    "The output for the FTW model is a raster with three classes as bands: field, field_boundaries, and non_field_background.  \n",
    "\n",
    "We will run a seperate flow to convert the fields and field boundaries into vector geometries.  Converting these results to geometries allows us to more easily post process the results or join the results with other vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0084f2a-74e0-4191-ad90-fb7047b564fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the classes that are predicted by the model\n",
    "model_features = result_mosaic['band'].data.tolist()\n",
    "\n",
    "# Remove the 'non_field_background' class for vectorization\n",
    "vector_features = [f for f in model_features if f != 'non_field_background']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f41cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this should take about 5 minutes to complete\n",
    "vectorized_results = rf_client.vectorize_mosaic(\n",
    "        store = model_outputs,\n",
    "        features = vector_features,\n",
    "        threshold = 0.5,\n",
    "        vectorize_method = VectorizeMethodEnum.SEMANTIC_SEGMENTATION_RASTERIO,\n",
    "        vectorize_config={\"stats\": True, \"medial_skeletonize\": False}\n",
    "    )\n",
    "\n",
    "print(vectorized_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd7fcd6-19fa-49f8-bb03-de062f39f4ca",
   "metadata": {},
   "source": [
    "## Save the vectorized results to the catalog\n",
    "We can store these vectorized outputs in the catalog by using WherobotsDB to persist the GeoParquet results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954fa1e-7441-45bb-96bd-2cb8f70aa3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "config = SedonaContext.builder().getOrCreate()\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5107085f-c3fe-4c00-8810-22c6247139f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\"CREATE DATABASE IF NOT EXISTS org_catalog.ftw_db\")\n",
    "\n",
    "vectorize_result_df = sedona.read.format(\"geoparquet\").load(vectorized_results)\n",
    "vectorize_result_df = vectorize_result_df.withColumnRenamed(\"label\", \"layer\")\n",
    "vectorize_result_df.writeTo(\"org_catalog.ftw_db.ftw_vectorized\").createOrReplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4571d",
   "metadata": {},
   "source": [
    "## Visualize the vectorized results\n",
    "To visualize the vectorized results, we will show the fields around Plymell, Kansas and filter out results with a score lower than 0.5. This threshold was determined through observation to strike a balance: it eliminates obvious noise without being overly aggressive, ensuring that we don't accidentally filter out too many relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark.sql.st_constructors import ST_GeomFromText\n",
    "from sedona.spark.sql.st_predicates import ST_Intersects\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Bounding box for Plymell, Kansas\n",
    "plymell_wkt = \"POLYGON ((-100.98 37.83, -100.98 37.68, -100.8 37.68, -100.8 37.83, -100.98 37.83))\"\n",
    "vectorize_result_filtered_df = vectorize_result_df.filter(\"layer == 'field'\").filter(\"score_mean > 0.1\") \\\n",
    "    .filter(ST_Intersects(f.col(\"geometry\"), ST_GeomFromText(lit(plymell_wkt))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9bb59f-0d2a-4def-9c93-51a0d73905ef",
   "metadata": {},
   "source": [
    "We will apply some WherobotsDB vector post processing operations to refine the model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc74553-d0ae-4d9d-a739-5e0c50f76c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 0.00001\n",
    "decimal_places = 6  # roughly 10cm in lat/lon\n",
    "df = vectorize_result_filtered_df.withColumn(\"geometry\", f.expr(\"ST_MakeValid(geometry)\"))\n",
    "df = df.withColumn(\"geometry\", f.expr(\"ST_SetSRID(geometry, 4326)\")) # vectorize_mosaic defaults to returning geometries in the CRS EPSG:4326\n",
    "df = df.withColumn(\"geometry\", f.expr(f\"ST_ReducePrecision(geometry, {decimal_places})\"))\n",
    "df = df.withColumn(\"geometry\", f.expr(f\"ST_SimplifyPreserveTopology(geometry, {tolerance})\"))\n",
    "df = df.repartition(200)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be0becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark.maps.SedonaKepler import SedonaKepler\n",
    "map = SedonaKepler.create_map(df=df, name=\"Vectorized results\")\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217c902",
   "metadata": {},
   "source": [
    "## Generate PM Tiles for visualization\n",
    "To improve visualization performance of a large number of geometries, we can use Wherobots built-in high performance PM tile generator.\n",
    "\n",
    "The FTW model has a tendency to create extremely large boundary geometries, which doesn't play nicely with PMTiles. To avoid this we subdivide the boundary geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d346fc22058fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_df = df.where(\"layer = 'field'\")\n",
    "# 1266 is the vertex count of the second-largest boundary geometry.\n",
    "# We use this instead of the absolute largest boundary (an extreme outlier)\n",
    "# so the subdivision threshold is high enough to keep most geometries intact\n",
    "# while still forcing that outlier-sized boundary to be subdivided for\n",
    "# better PMTiles rendering performance.\n",
    "boundaries_df = df.where(\"layer = 'field_boundaries'\").withColumn(\"geometry\", f.expr(\"ST_SubDivideExplode(geometry, 1266)\"))\n",
    "tile_features_df = fields_df.withColumn(\"layer\", f.lit(\"fields\")).unionByName(\n",
    "    boundaries_df.withColumn(\"layer\", f.lit(\"boundaries\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6919ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wherobots import vtiles\n",
    "full_tiles_path = os.getenv(\"USER_S3_PATH\") + \"tiles.pmtiles\"\n",
    "vtiles.generate_pmtiles(tile_features_df, full_tiles_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec582b8a-082b-4bfd-a85f-22c4bb69718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtiles.show_pmtiles(full_tiles_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61980171-a74d-4849-a77a-34fad589c879",
   "metadata": {},
   "source": [
    "## Sharing PMTiles results with the Wherobots PMTiles Viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0832d69-a594-4f1d-8c3b-a9692f95bf71",
   "metadata": {},
   "source": [
    "You can generate a pre-signed url to your pmtiles using `get_url`.\n",
    "\n",
    "Then, copy this to your clipboard with right-click + \"Copy Output to Clipboard\".\n",
    "\n",
    "You can paste this url into https://tile-viewer.wherobots.com/ and create a publicly accessible PMTiles map served from your own bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8444bd-e4c9-4c10-9c89-386c5d48c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wherobots.tools.utility.s3_utils import get_url\n",
    "\n",
    "get_url(full_tiles_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e79110",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. **Kerner, H., Chaudhari, S., Ghosh, A., Robinson, C., Ahmad, A., Choi, E., Jacobs, N., Holmes, C., Mohr, M., et al. (2024).** Fields of The World: A Machine Learning Benchmark Dataset For Global Agricultural Field Boundary Segmentation. *arXiv preprint arXiv:2409.16252*. Accepted at AAAI-2025 Artificial Intelligence for Social Impact (AISI) track.\n",
    "2. ESA. (2015). Sentinel-2 User Handbook (Issue 1, Rev. 2). European Space Agency. https://sentinels.copernicus.eu/documents/247904/685211/Sentinel-2_User_Handbook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
