{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75016dba-3cff-4563-905e-3ea34976df7a",
   "metadata": {},
   "source": [
    "<img src=\"assets/img/chesapeakersc-banner.jpg\">\n",
    "\n",
    "# Detecting roads with RasterFlow\n",
    "\n",
    "This notebook will guide you through detecting roads from aerial imagery, using Wherobots RasterFlow and the ChesapeakeRSC model. You will gain a hands-on understanding of how to run models like ChesapeakeRSC on your selected area of interest, vectorize the model outputs, and work with those vectors in WherobotsDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76561f61",
   "metadata": {},
   "source": [
    "### ChesapeakeRSC\n",
    "\n",
    "The [ChesapeakeRSC](https://github.com/isaaccorley/ChesapeakeRSC)<sup>1</sup> model is an open source segmentation model that can detect roads from high resolution imagery, even when those roads may be occluded by tree cover.\n",
    "\n",
    "This model predicts 2 classes: \n",
    "- background\n",
    "- road\n",
    "\n",
    "It was trained on high resolution imagery from the [National Agriculture Imagery Program (NAIP)](https://www.usgs.gov/centers/eros/science/usgs-eros-archive-aerial-photography-national-agriculture-imagery-program-naip) and labeled land cover data for the state of Maryland in the United States. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a65529",
   "metadata": {},
   "source": [
    "## Selecting an Area of Interest (AOI)\n",
    "We will choose an Area of Interest (AOI) for our analysis. ChesapeakeRSC was trained on select geographies in the Northeastern United States.  \n",
    "\n",
    "We will try it out in Montgomery County, Maryland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7139d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wkls\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "# Generate a geometry for Montgomery County, Maryland using Well-Known Locations (https://github.com/wherobots/wkls)\n",
    "gdf = gpd.read_file(wkls['us']['md']['Montgomery County'].geojson())\n",
    "\n",
    "# Save the geometry to a parquet file in the user's S3 path\n",
    "aoi_path = os.getenv(\"USER_S3_PATH\") + \"montgomery.parquet\"\n",
    "gdf.to_parquet(aoi_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e05f2",
   "metadata": {},
   "source": [
    "## Initializing the RasterFlow client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from rasterflow_remote import RasterflowClient\n",
    "\n",
    "from rasterflow_remote.data_models import (\n",
    "    ModelRecipes, \n",
    "    VectorizeMethodEnum\n",
    ")\n",
    "\n",
    "rf_client = RasterflowClient(cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e414dee",
   "metadata": {},
   "source": [
    "## Running a model\n",
    "\n",
    "RasterFlow has pre-defined workflows to simplify orchestration of the processing steps for model inference.  These steps include:\n",
    "* Ingesting imagery for the specified Area of Interest (AOI)\n",
    "* Generating a seamless image from multiple image tiles (a mosaic) \n",
    "* Running inference with the selected model\n",
    "\n",
    "The output is a Zarr store of the model outputs.\n",
    "\n",
    "Note: This step will take approximately 30 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b64719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = rf_client.build_and_predict_mosaic_recipe(\n",
    "    # Path to our AOI in GeoParquet or GeoJSON format\n",
    "    aoi = aoi_path,\n",
    "\n",
    "    # Date range for imagery to be used by the model\n",
    "    start = datetime(2017, 1, 1),\n",
    "    end = datetime(2018, 1, 1),\n",
    "\n",
    "    # Coordinate Reference System EPSG code for the output mosaic   \n",
    "    crs_epsg = 3857,\n",
    "\n",
    "    # The model recipe to be used for inference (ChesapeakeRSC in this case)\n",
    "    model_recipe = ModelRecipes.CHESAPEAKE_RSC\n",
    ")\n",
    "\n",
    "print(model_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d35610",
   "metadata": {},
   "source": [
    "## Visualize a subset of the model outputs\n",
    "We will use hvplot and datashader to visualize a small subset of the model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39a48d-aa04-4ef5-a149-9344dea37494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for visualization and coordinate transformation\n",
    "import hvplot.xarray\n",
    "import xarray as xr\n",
    "import s3fs \n",
    "import zarr\n",
    "from pyproj import Transformer\n",
    "from holoviews.element.tiles import EsriImagery \n",
    "\n",
    "# Open the Zarr store\n",
    "fs = s3fs.S3FileSystem(profile=\"default\", asynchronous=True)\n",
    "zstore = zarr.storage.FsspecStore(fs, path=model_outputs[5:])\n",
    "ds = xr.open_zarr(zstore)\n",
    "\n",
    "# Create a transformer to convert from lat/lon to meters\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3857\", always_xy=True)\n",
    "\n",
    "# Transform bounding box coordinates from lat/lon to meters\n",
    "min_lon = -77.07\n",
    "max_lon = -77.05\n",
    "min_lat = 39.17\n",
    "max_lat = 39.19\n",
    "\n",
    "(min_x, max_x), (min_y, max_y) = transformer.transform(\n",
    "    [min_lon, max_lon], \n",
    "    [min_lat, max_lat]\n",
    ")\n",
    "\n",
    "# Select the height variable and slice the dataset to the bounding box\n",
    "# y=slice(max_y, min_y) handles the standard \"North-to-South\" image orientation\n",
    "ds_subset = ds.sel(band=\"road\",\n",
    "    x=slice(min_x, max_x), \n",
    "    y=slice(max_y, min_y) \n",
    ")\n",
    "\n",
    "# Select the first time step and extract the variables array\n",
    "arr_subset = ds_subset.isel(time=0)[\"variables\"]\n",
    "\n",
    "# Create a base map layer using Esri satellite imagery\n",
    "base_map = EsriImagery()\n",
    "\n",
    "# Create an overlay layer from the model outputs with hvplot\n",
    "output_layer = arr_subset.hvplot(\n",
    "    x = \"x\",\n",
    "    y = \"y\",\n",
    "    geo = True,           # Enable geographic plotting\n",
    "    dynamic = True,       # Enable dynamic rendering for interactivity\n",
    "    rasterize = True,     # Use datashader for efficient rendering of large datasets\n",
    "    cmap = \"viridis\",     # Color map for visualization\n",
    "    aspect = \"equal\",     # Maintain equal aspect ratio\n",
    "    title = \"ChesapeakeRSC Model Outputs\" \n",
    ").opts(\n",
    "    width = 600, \n",
    "    height = 600,\n",
    "    alpha = .7           # Set transparency to see the base map underneath\n",
    ")\n",
    "\n",
    "# Combine the base map and output layer\n",
    "final_plot = base_map * output_layer\n",
    "final_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4106b",
   "metadata": {},
   "source": [
    "## Vectorize the raster model outputs\n",
    "The output for the ChesapeakeRSC model is a raster with two classes: background and road.  \n",
    "\n",
    "We can run a seperate flow to convert the roads into vector geometries, based on the confidence threshold.  Converting these results to geometries allows us to more easily post process the results or join the resuilts with other vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1029e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the classes that are predicted by the model\n",
    "model_features = xr.open_zarr(zstore)['band'].data.tolist()\n",
    "\n",
    "# Remove the 'background' class for vectorization\n",
    "vector_features = [f for f in model_features if f != 'background']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f41cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this should take about 5 minutes to complete\n",
    "vectorized_results = rf_client.vectorize_mosaic(\n",
    "        store = model_outputs,\n",
    "        features = vector_features,\n",
    "        threshold = 0.5,\n",
    "        vectorize_method = VectorizeMethodEnum.SEMANTIC_SEGMENTATION_RASTERIO,\n",
    "        vectorize_config={\"stats\": True, \"medial_skeletonize\": False}\n",
    "    )\n",
    "\n",
    "print(vectorized_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd7fcd6-19fa-49f8-bb03-de062f39f4ca",
   "metadata": {},
   "source": [
    "## Save the vectorized results to the catalog\n",
    "We can store these vectorized outputs in the catalog by using WherobotsDB to persist the GeoParquet results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954fa1e-7441-45bb-96bd-2cb8f70aa3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "\n",
    "config = SedonaContext.builder().getOrCreate()\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5107085f-c3fe-4c00-8810-22c6247139f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.sql(\"CREATE DATABASE IF NOT EXISTS org_catalog.chesapeake_db\")\n",
    "\n",
    "df = sedona.read.format(\"geoparquet\").load(vectorized_results)\n",
    "df = df.withColumnRenamed(\"label\", \"layer\")\n",
    "df.writeTo(\"org_catalog.chesapeake_db.chesapeake_vectorized\").createOrReplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4571d",
   "metadata": {},
   "source": [
    "## Visualize the vectorized results\n",
    "\n",
    "To visualize the vectorized results, we will choose a small subset of the outputs that intersect with a bounding box around the University of Maryland campus.  We will also filter out results with a score lower than 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark.sql.st_constructors import ST_MakeEnvelope\n",
    "from sedona.spark.sql.st_predicates import ST_Intersects\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "df = df.filter(\n",
    "    ST_Intersects(f.col(\"geometry\"), ST_MakeEnvelope(min_lon, min_lat, max_lon, max_lat))\n",
    ").filter(\"score_mean > 0.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be0becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark.maps.SedonaKepler import SedonaKepler\n",
    "map = SedonaKepler.create_map(df=df, name=\"Vectorized results\")\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217c902",
   "metadata": {},
   "source": [
    "## Generate PM Tiles for visualization\n",
    "To improve visualization performance of a large number of geometries, we can use Wherobots built-in high performance PM tile generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6919ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wherobots import vtiles\n",
    "\n",
    "full_tiles_path = os.getenv(\"USER_S3_PATH\") + \"tiles.pmtiles\"\n",
    "vtiles.generate_pmtiles(df, full_tiles_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec582b8a-082b-4bfd-a85f-22c4bb69718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtiles.show_pmtiles(full_tiles_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45a674-e4b4-420b-8daa-331e7ad99e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tiles_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693029c",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. **Robinson, C., Corley, I., Ortiz, A., Dodhia, R., Lavista Ferres, J. M., & Najafirad, P. (2024).** Seeing the roads through the trees: A benchmark for modeling spatial dependencies with aerial imagery. *arXiv preprint arXiv:2401.06762*. https://arxiv.org/abs/2401.06762"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
