{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87799ae3-0d1a-4ba8-b8b8-8be64845f31b",
   "metadata": {},
   "source": [
    "# Accelerating Geospatial Datasets\n",
    "\n",
    "This notebook will show you how to optimize the way you work with large geospatial datasets — using Wherobots to efficiently manage, cluster, and export spatial data for high-performance analysis and downstream use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503de501-f967-4fbb-91df-7b922dca66b0",
   "metadata": {},
   "source": [
    "## What you will learn\n",
    "\n",
    "This notebook will teach you to:\n",
    "\n",
    "* Write DataFrames as managed tables in WherobotsDB\n",
    "* Apply spatial clustering to improve query performance on large datasets\n",
    "* Export geospatial tables to Geoparquet for sharing or external processing\n",
    "* Confirm export success and inspect output\n",
    "* Use both Python and SQL workflows for data export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777dc09-52b7-4db1-8a8f-9a84d98d5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import SedonaContext\n",
    "\n",
    "config = SedonaContext.builder() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025410b",
   "metadata": {},
   "source": [
    "# Working with tabular data in Wherobots\n",
    "\n",
    "Wherobots supports loading structured tabular data directly from cloud storage. In this example, we are working with the GDELT dataset — a global event database published as daily CSV files on AWS S3.\n",
    "\n",
    "When working with your own data, you can:\n",
    "\n",
    "* Load data into Wherobots Cloud managed storage ([Docs](https://docs.wherobots.com/latest/develop/storage-management/managed-storage/?h=managed+st))\n",
    "* Connect directly to cloud storage like AWS S3 ([Docs](https://docs.wherobots.com/latest/develop/storage-management/s3-storage-integration/?h=s3))\n",
    "\n",
    "We’ll start by reading the GDELT data in its raw CSV format into a Sedona DataFrame.\n",
    "\n",
    "The GDELT dataset uses tab-separated values (TSV), so we specify the tab character (\\t) as the delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef09e5f-d74b-46f8-a00a-c8fbec2da4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 's3://gdelt-open-data/events/*.*.csv'\n",
    "\n",
    "csv_df = sedona.read.format(\"csv\") \\\n",
    "    .option(\"delimiter\", \"\\\\t\") \\\n",
    "    .load(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e37e05-e190-4570-b748-a48b277b9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch the header file from the URL\n",
    "response = requests.get('https://gdeltproject.org/data/lookups/CSV.header.dailyupdates.txt')\n",
    "response.raise_for_status()  # ensure we notice bad responses\n",
    "\n",
    "# Assume the first line contains the header names and they're comma-separated\n",
    "header_line = response.text.splitlines()[0].strip()\n",
    "headers = header_line.split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac074a6-e142-49a3-8d99-e7af2dc13392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the headers\n",
    "csv_df = csv_df.toDF(*headers)\n",
    "csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37aa8d7-f176-47b0-b004-dff7534bc8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of rows\n",
    "csv_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9e3cf",
   "metadata": {},
   "source": [
    "## Creating a managed table from raw data\n",
    "\n",
    "We can now convert the DataFrame into an Iceberg table. The steps to complete this are as follows:\n",
    "\n",
    "1. Register the DataFrame as a **temporary view** in Wherobots.\n",
    "\n",
    "> Temporary views let you reference DataFrames in SQL without writing them to disk.\n",
    "\n",
    "2. Create a Database Schema for our Iceberg database.\n",
    "\n",
    "> The `CREATE DATABASE` command creates the `wherobots.gdelt` database if it doesn’t already exist.\n",
    "\n",
    "3. Create a managed table in Wherobots by selecting data from the temporary view. As part of this step, we also create a **geometry column** using the latitude and longitude fields from the CSV.\n",
    "\n",
    "> We use `ST_Point` to create a point geometry from the latitude and longitude, and `ST_SetSRID` to set the spatial reference system to **EPSG:4326** (WGS 84)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c477e-3088-4c79-8472-605d1660f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view to create our table from the DataFrame\n",
    "csv_df.createOrReplaceTempView('csv_df')\n",
    "\n",
    "name = 'gdelt'\n",
    "\n",
    "# Create a Database\n",
    "sedona.sql(f'''\n",
    "CREATE DATABASE IF NOT EXISTS wherobots.{name}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090da2c8-b8b7-4f2f-9578-e4dc3dbf135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Havasu table and create a geometry\n",
    "sedona.sql(f'''\n",
    "CREATE OR REPLACE TABLE wherobots.{name}.gdelt AS \n",
    "SELECT *, \n",
    "ST_SetSRID(\n",
    "    ST_Point(ActionGeo_Long, ActionGeo_Lat),\n",
    "4326) as geometry\n",
    "FROM csv_df\n",
    "LIMIT 10000\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210d8e3",
   "metadata": {},
   "source": [
    "# Writing efficient GeoParquet with metadata\n",
    "\n",
    "When exporting spatial data for downstream use, the GeoParquet format offers an efficient, interoperable way to store vector data with embedded spatial metadata.\n",
    "\n",
    "GeoParquet builds on the Parquet columnar format, adding metadata for geometries, coordinate reference systems (CRS), and bounding boxes.\n",
    "\n",
    "> For more on GeoParquet, see the [GeoParquet](https://geoparquet.org/) specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587febf6-5751-47c8-9915-1e6f3ce5409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user_uri = os.getenv(\"USER_S3_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f6e5cc",
   "metadata": {},
   "source": [
    "## Partitioning and adding bounding boxes\n",
    "\n",
    "Before writing the data, we optimize it for efficient storage and querying:\n",
    "\n",
    "* **GeoHash partitioning** — We compute a GeoHash for each geometry and partition the data accordingly.\n",
    "This organizes the dataset spatially, improving query performance for spatial ranges.\n",
    "\n",
    "* **Bounding box metadata** — We add a bounding box for each geometry, allowing readers to perform fast spatial filtering without loading the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f5a4a-c3da-481d-8478-73ecc1c059e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize the table by the GeoHash for improved partitioning\n",
    "\n",
    "gdelt = sedona.sql(f'''SELECT \n",
    "*,\n",
    "ST_GeoHash(geometry, 15) AS geohash,\n",
    "struct(st_xmin(geometry) as xmin, st_ymin(geometry) as ymin, st_xmax(geometry) as xmax, st_ymax(geometry) as ymax) as bbox\n",
    "FROM wherobots.{name}.gdelt''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ddf411",
   "metadata": {},
   "source": [
    "## Writing the GeoParquet file\n",
    "\n",
    "We write the data using the **GeoParquet** format with key options:\n",
    "\n",
    "* `geoparquet.version` — Specifies the format version (recommended: `1.1.0`)\n",
    "* `geoparquet.covering` — Defines the spatial covering method (we use `bbox`)\n",
    "* `geoparquet.crs` — Passes the PROJJSON metadata for the CRS (optional)\n",
    "* `compression` — We apply `snappy` compression for efficient storage\n",
    "\n",
    "The data is repartitioned by `geohash` and sorted within partitions to improve downstream query performance:\n",
    "\n",
    "> This writes a partitioned, metadata-rich GeoParquet dataset ready for scalable spatial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7cc5d-23b5-4359-84d1-bcba410da18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdelt.repartitionByRange(30, \"geohash\") \\\n",
    "    .sortWithinPartitions(\"geohash\") \\\n",
    "    .drop(\"geohash15\") \\\n",
    "    .write \\\n",
    "    .format(\"geoparquet\") \\\n",
    "    .option(\"geoparquet.version\", \"1.1.0\") \\\n",
    "    .option(\"geoparquet.covering\", \"bbox\") \\\n",
    "    .save(user_uri + \"gdelt-snappy\", mode='overwrite', compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
