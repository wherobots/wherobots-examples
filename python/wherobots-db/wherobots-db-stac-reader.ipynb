{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8482b9b9-03f6-4517-bffa-7fb2c7ce4200",
   "metadata": {},
   "source": [
    "![](https://wherobots.com/wp-content/uploads/2023/12/Inline-Blue_Black_onWhite@3x.png)\n",
    "\n",
    "# Introduction to `STAC Reader and API` and `STAC ` for WherobotsDB\n",
    "\n",
    "In this notebook, we will demonstrate how to load SpatioTemporal Asset Catalogs (STAC) collections in WherobotsDB. We will cover the following steps:  \n",
    "\n",
    "- Connecting to a STAC API: Learn how to establish a connection to a STAC API endpoint.\n",
    "- Searching items and load them into WherobotsDB: See how to load STAC collections into WherobotsDB for further analysis.\n",
    "- Applying Spatial and Temporal Filters: Learn to filter the data based on spatial and temporal criteria to focus on specific areas and time periods.\n",
    "- Saving Data: Discover how to save the filtered data into various formats for further use.\n",
    "\n",
    "By the end of this notebook, you will be able to efficiently manage and analyze geospatial data using STAC collections in WherobotsDB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f646f-53dc-4594-a240-099971d0c9ed",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "    <img src=\"https://stacspec.org/public/images-original/STAC-01.png\" alt=\"Image 2\" width=\"300\" height=\"100\">\n",
    "</div>\n",
    "\n",
    "The STAC data source enables seamless integration with [SpatioTemporal Asset Catalog (STAC)](https://stacspec.org/) APIs, allowing users to efficiently read and interact with geospatial data. This data source supports reading both STAC items and collections, making it a versatile tool for various geospatial data processing tasks.  To utilize the STAC data source, you can load a STAC catalog into a Sedona DataFrame using the stac format. The path can be either a local STAC collection JSON file or an HTTP/HTTPS endpoint to retrieve the collection JSON file. This flexibility allows for easy access to both locally stored and remotely hosted STAC data.  \n",
    "\n",
    "### Technical Details:\n",
    "- STAC API Integration: Connect to any STAC-compliant API to fetch and process geospatial data.\n",
    "- DataFrame Support: Load STAC data directly into a Sedona DataFrame for further analysis and processing using Spark.\n",
    "- Flexible Input Paths: Accepts both local file paths and remote URLs, providing versatility in data sourcing.\n",
    "\n",
    "### Potential Use Cases:\n",
    "- Geospatial Data Analysis: Perform complex spatial queries and analyses on large geospatial datasets.\n",
    "- Environmental Monitoring: Access and analyze satellite imagery and other remote sensing data for environmental studies.\n",
    "- Urban Planning: Utilize geospatial data to support urban development and infrastructure planning.\n",
    "- Disaster Response: Quickly access and process geospatial data to aid in disaster response and recovery efforts.\n",
    "\n",
    "### Find STAC Service Endpoints\n",
    "\n",
    "- [STAC Browser](https://radiantearth.github.io/stac-browser/#/?.language=en)\n",
    "- [AWS Earth Search](https://earth-search.aws.element84.com/v1/collections/sentinel-2-pre-c1-l2a)\n",
    "- [EarthHub](https://earthdatahub.destine.eu/api/stac/v1/collections/copernicus-dem)\n",
    "- [Google APIS](https://storage.googleapis.com/cfo-public/vegetation/collection.json)\n",
    "- [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip)\n",
    "\n",
    "By leveraging the STAC data source, users can efficiently manage and analyze vast amounts of geospatial data, unlocking new insights and applications across various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c74bb-7414-4c4d-a835-95b7389ca5bf",
   "metadata": {},
   "source": [
    "## Initial Configuration\n",
    "\n",
    "Note that many STAC datasources are hosted on AWS S3, and to configure Apache Sedona for anonymous access to an Amazon S3 bucket, you can set the spark.hadoop.fs.s3a.bucket.<bucket-name>.aws.credentials.provider property to org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider. This setting directs Sedona to use the AnonymousAWSCredentialsProvider, enabling access to publicly accessible S3 buckets without requiring AWS credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a4406-ac0c-4196-868f-c288ca6dff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "\n",
    "config = SedonaContext.builder() \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.bucket.e84-earth-search-sentinel-data.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\\\n",
    "    .getOrCreate()\n",
    "sedona = SedonaContext.create(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a04d5f-e7e6-41d5-942a-e353938af27c",
   "metadata": {},
   "source": [
    "### Load a STAC collection from an HTTP/HTTPS endpoint:\n",
    "\n",
    "This code below uses Apache Sedona and PySpark to load STAC data from the Sentinel-2 collection via a specified URL. It then explodes the “assets” map into key-value pairs, extracting the “href” and “rast” fields from the “value” struct. The resulting DataFrame is ordered by the “datetime” field in descending order. Finally, it counts the number of rows in the processed DataFrame to verify the output.\n",
    "\n",
    "Note: if you need to load the image using the out-db reader from S3, you need to setup the correct S3 credential provider in Sedona config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bbab0c-ad4b-43f1-9ac6-2a826a216cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, map_values\n",
    "\n",
    "# Load from STAC datasource\n",
    "df = sedona.read.format(\"stac\") \\\n",
    "    .load(\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-c1-l2a\") \\\n",
    "    .orderBy(col(\"datetime\").desc())\n",
    "df.printSchema() \n",
    "\n",
    "# Explode the map into key-value pairs\n",
    "df_exploded = df.select(\"id\", \"datetime\", explode(\"assets\").alias(\"key\", \"value\"))\n",
    "\n",
    "# Select the 'rast' field from the 'value' struct\n",
    "df_rast = df_exploded.select(\n",
    "    col(\"id\"), \n",
    "    col(\"datetime\"), \n",
    "    col(\"key\"), \n",
    "    col(\"value.href\").alias(\"href\"), \n",
    "    col(\"value.rast\").alias(\"rast\")\n",
    ")\n",
    "\n",
    "# Show the loaded raster DataFrame\n",
    "df_rast.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38f49a-1f0f-4005-a9a9-4cf642858487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T20:46:50.310191Z",
     "iopub.status.busy": "2025-01-30T20:46:50.309867Z",
     "iopub.status.idle": "2025-01-30T20:46:50.313158Z",
     "shell.execute_reply": "2025-01-30T20:46:50.312548Z",
     "shell.execute_reply.started": "2025-01-30T20:46:50.310166Z"
    }
   },
   "source": [
    "## Integrate with OutDB Raster\n",
    "\n",
    "This code filters the df_rast DataFrame to select rows where the href column ends with .tif, limits the results to 4 rows, and converts the rast field into an image using the RS_AsImage function. The resulting DataFrame, which contains the generated images, is then displayed using SedonaUtils.display_image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3299827e-557f-425b-ba73-084e26b5441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Filter the DataFrame for '.tif' hrefs and limit to 4 items\n",
    "dfImage = df_rast.filter(F.col(\"href\").endswith(\".tif\")) \\\n",
    "    .limit(4) \\\n",
    "    .selectExpr(\"RS_AsImage(rast, 300) as raster_image1\")\n",
    "\n",
    "# Display the image\n",
    "SedonaUtils.display_image(dfImage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1aa5b-5609-4a63-93c7-226ec5d03cdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:20:06.728140Z",
     "iopub.status.busy": "2025-02-05T16:20:06.727817Z",
     "iopub.status.idle": "2025-02-05T16:20:06.732207Z",
     "shell.execute_reply": "2025-02-05T16:20:06.731442Z",
     "shell.execute_reply.started": "2025-02-05T16:20:06.728115Z"
    }
   },
   "source": [
    "## Python STAC API\n",
    "\n",
    "The STAC data source can also be loaded and searched using a new Python API. It will follow PySTAC client behavior. \n",
    "\n",
    "This Python code initializes a STAC client by importing the Client class from the sedona.stac.client module and the DataFrame class from pyspark.sql. It then opens the client to connect to the Earth Search API using the specified URL (https://earth-search.aws.element84.com/v1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269e1d9-bdf1-48e5-b0da-63e9cf42cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.stac.client import Client\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Initialize the client\n",
    "client = Client.open(\"https://earth-search.aws.element84.com/v1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28371a70-8a12-42d7-85e7-6a9c173e2fcc",
   "metadata": {},
   "source": [
    "This Python code uses the client.search() method to query the “sentinel-2-c1-l2a” collection from the STAC API, filtering results for items from the year 2025. It sets the return_dataframe parameter to False, meaning the search results will not be returned as a DataFrame. Finally, it prints the retrieved items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd84121-7a0f-4a89-ba76-bd70c6438bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search items on a collection within a year\n",
    "items = client.search(\n",
    "    collection_id=\"sentinel-2-c1-l2a\",\n",
    "    datetime=\"2025\",\n",
    "    return_dataframe=False\n",
    ")\n",
    "\n",
    "# Print the count of items\n",
    "items_list = list(items)\n",
    "print(f\"Loaded items: {len(items_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ccae91-3471-4d9d-aa73-04f2ea6028f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search items with bounding box and interval\n",
    "items = client.search(\n",
    "    collection_id=\"sentinel-2-c1-l2a\",\n",
    "    bbox=[-180.0, -90.0, 180.0, 90.0],\n",
    "    datetime=\"2025\",\n",
    "    return_dataframe=False\n",
    ")\n",
    "\n",
    "# Print the count of items\n",
    "items_list = list(items)\n",
    "print(f\"Loaded items: {len(items_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297bf0ba-cb17-4610-8d14-1993e76e9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search multiple items with multiple bounding boxes\n",
    "bbox_list = [\n",
    "    [-180.0, -90.0, 180.0, 90.0],\n",
    "    [-100.0, -50.0, 100.0, 50.0]\n",
    "]\n",
    "item_df = client.search(\n",
    "    collection_id=\"sentinel-2-c1-l2a\",\n",
    "    bbox=bbox_list,\n",
    "    return_dataframe=True\n",
    ")\n",
    "\n",
    "# Show the datafram\n",
    "item_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fea083-54ab-48ae-a981-83673db51f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T16:24:23.392668Z",
     "iopub.status.busy": "2025-02-05T16:24:23.392346Z",
     "iopub.status.idle": "2025-02-05T16:24:23.396945Z",
     "shell.execute_reply": "2025-02-05T16:24:23.396283Z",
     "shell.execute_reply.started": "2025-02-05T16:24:23.392644Z"
    }
   },
   "source": [
    "## Save to STAC GeoParquet format\n",
    "\n",
    "We also implement a stac df to stac geoparquet converter that can be used to write geoparquet without requiring \n",
    "users to explicitly cast the schema or explode the dataframe. This could potentially be implemented in \n",
    "stac_geopaquet writer using a stac df loaded if feasible.\n",
    "\n",
    "This code connects to the Microsoft Planetary Computer’s STAC API to fetch the “aster-l1t” collection. It defines spatial (bounding box) and temporal (datetime interval) extents for the data to be saved. The script checks if a specified output path (/tmp/stac_temp_aster-l1t) exists, deletes it if it does, and then saves the filtered items to GeoParquet. Afterward, it reads the saved GeoParquet data into a Spark DataFrame and displays the contents. Finally, it cleans up by deleting the output path again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b75d06-82a6-4802-b995-1518e96ebd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save items in DataFrame to GeoParquet with both bounding boxes and intervals\n",
    "client_stac = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "collection = client_stac.get_collection(\"aster-l1t\")\n",
    "\n",
    "# Define spatial and temporal extents\n",
    "bbox = [[-180.0, -90.0, 180.0, 90.0]]\n",
    "datetime = [[\"2006-01-01T00:00:00Z\", \"2007-01-01T00:00:00Z\"]]\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "output_path = \"/tmp/stac_temp_aster-l1t\"\n",
    "\n",
    "# Delete the out_path if it exists\n",
    "if os.path.exists(out_path):\n",
    "    shutil.rmtree(out_path)\n",
    "\n",
    "# Save items to GeoParquet\n",
    "collection.save_to_geoparquet(\n",
    "    output_path=out_path, bbox=bbox, datetime=datetime\n",
    ")\n",
    "\n",
    "# Delete the out_path if it exists\n",
    "if os.path.exists(out_path):\n",
    "    shutil.rmtree(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccaa88d-448f-4255-9b5f-910f350e84b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
